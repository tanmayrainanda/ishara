{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":52950,"databundleVersionId":5973250,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install python-levenshtein tqdm pyarrow wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:53:03.524994Z","iopub.execute_input":"2024-11-29T08:53:03.525496Z","iopub.status.idle":"2024-11-29T08:53:13.921986Z","shell.execute_reply.started":"2024-11-29T08:53:03.525440Z","shell.execute_reply":"2024-11-29T08:53:13.920998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport random\nimport math\nfrom typing import List, Optional, Tuple, Dict\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nimport Levenshtein\nfrom tqdm.auto import tqdm\nimport time\nimport pyarrow.parquet as pq\nimport wandb\nimport sys\n\nwandb.login(key=\"afe8b8c0a3f1c1339a3daa9f619cb7c311218022\")\nwandb.init(project=\"asl-translation\")\n\n# Constants\nLPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\nFRAME_LEN = 128\n\ndef create_feature_columns():\n    X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\n    Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\n    Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n    return X + Y + Z\n\nFEATURE_COLUMNS = create_feature_columns()\n\nclass ASLTokenizer:\n    \"\"\"Tokenizer for ASL fingerspelling sequences\"\"\"\n    def __init__(self, vocab_path: str):\n        with open(vocab_path, 'r') as f:\n            self.char_to_idx = json.load(f)\n            \n        # Add special tokens\n        self.char_to_idx['P'] = 59  # Padding token\n        self.char_to_idx['<'] = 60  # Start token\n        self.char_to_idx['>'] = 61  # End token\n            \n        self.idx_to_char = {v: k for k, v in self.char_to_idx.items()}\n        self.vocab_size = len(self.char_to_idx)\n        \n    def encode(self, text: str) -> torch.Tensor:\n        \"\"\"Convert text to token indices\"\"\"\n        tokens = [self.char_to_idx['<']]  # Start token\n        for char in text:\n            tokens.append(self.char_to_idx.get(char, self.char_to_idx['P']))\n        tokens.append(self.char_to_idx['>'])  # End token\n        return torch.tensor(tokens)\n    \n    def decode(self, tokens: torch.Tensor) -> str:\n        \"\"\"Convert token indices to text\"\"\"\n        text = []\n        for token in tokens:\n            if token.item() == self.char_to_idx['>']:\n                break\n            if token.item() not in [self.char_to_idx['P'], self.char_to_idx['<']]:\n                text.append(self.idx_to_char[token.item()])\n        return ''.join(text)\n\ndef preprocess_data(\n    data_dir: str,\n    metadata_path: str,\n    output_dir: str,\n    chunk_size: int = 1000\n):\n    \"\"\"Preprocess the data and save as TFRecords\"\"\"\n    print(\"Loading metadata...\")\n    df = pd.read_csv(metadata_path)\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create sequence to file mapping\n    print(\"Creating sequence index...\")\n    sequence_map = {}\n    for parquet_file in tqdm(list(Path(data_dir).glob('*.parquet'))):\n        table = pq.read_table(parquet_file, columns=['sequence_id'])\n        sequences = pd.unique(table['sequence_id'].to_numpy())\n        for seq_id in sequences:\n            sequence_map[seq_id] = str(parquet_file)\n    \n    # Filter sequences\n    df = df[df['sequence_id'].isin(sequence_map.keys())]\n    \n    # Process data in chunks and save as TFRecords\n    num_chunks = (len(df) + chunk_size - 1) // chunk_size\n    \n    for chunk_idx in range(num_chunks):\n        chunk_start = chunk_idx * chunk_size\n        chunk_end = min((chunk_idx + 1) * chunk_size, len(df))\n        chunk_df = df.iloc[chunk_start:chunk_end]\n        \n        tf_file = os.path.join(output_dir, f'chunk_{chunk_idx:04d}.tfrecord')\n        \n        with tf.io.TFRecordWriter(tf_file) as writer:\n            for _, row in tqdm(chunk_df.iterrows(), total=len(chunk_df), \n                             desc=f\"Processing chunk {chunk_idx+1}/{num_chunks}\"):\n                seq_id = row['sequence_id']\n                parquet_file = sequence_map[seq_id]\n                \n                # Read sequence data\n                table = pq.read_table(\n                    parquet_file,\n                    filters=[('sequence_id', '=', seq_id)]\n                )\n                seq_df = table.to_pandas()\n                \n                # Extract landmarks\n                landmark_cols = [col for col in seq_df.columns if col in FEATURE_COLUMNS]\n                frames = seq_df[landmark_cols].values\n                \n                # Calculate the number of NaN values in each hand landmark\n                right_hand_cols = [col for col in landmark_cols if 'right_hand' in col]\n                left_hand_cols = [col for col in landmark_cols if 'left_hand' in col]\n                \n                r_nonan = np.sum(np.sum(np.isnan(frames[:, [i for i, col in enumerate(landmark_cols) if col in right_hand_cols]]), axis=1) == 0)\n                l_nonan = np.sum(np.sum(np.isnan(frames[:, [i for i, col in enumerate(landmark_cols) if col in left_hand_cols]]), axis=1) == 0)\n                no_nan = max(r_nonan, l_nonan)\n                \n                if 2 * len(row['phrase']) < no_nan:\n                    # Create feature dictionary for TFRecord\n                    feature = {\n                        col: tf.train.Feature(\n                            float_list=tf.train.FloatList(value=frames[:, i]))\n                        for i, col in enumerate(landmark_cols)\n                    }\n                    feature['phrase'] = tf.train.Feature(\n                        bytes_list=tf.train.BytesList(value=[bytes(row['phrase'], 'utf-8')])\n                    )\n                    \n                    # Write to TFRecord\n                    example = tf.train.Example(features=tf.train.Features(feature=feature))\n                    writer.write(example.SerializeToString())\n    \n    # Save metadata\n    metadata = {\n        'num_chunks': num_chunks,\n        'chunk_size': chunk_size,\n        'total_sequences': len(df),\n        'feature_columns': FEATURE_COLUMNS\n    }\n    \n    with open(os.path.join(output_dir, 'metadata.json'), 'w') as f:\n        json.dump(metadata, f)\n    \n    print(f\"Preprocessing complete. {num_chunks} chunks saved to {output_dir}\")\n\nclass ASLDataset(Dataset):\n    def __init__(\n        self,\n        tf_records: List[str],\n        tokenizer,\n        max_len: int = FRAME_LEN,\n        augment: bool = True,\n        mode: str = 'train'\n    ):\n        self.tf_records = tf_records\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.augment = augment\n        self.mode = mode\n        \n        # Create TensorFlow dataset\n        self.dataset = tf.data.TFRecordDataset(self.tf_records)\n        self.dataset = self.dataset.map(self.decode_fn)\n        self.dataset = self.dataset.map(self.convert_fn)\n        self.dataset = self.dataset.cache()\n        \n        # Convert to PyTorch tensors\n        self.data = list(self.dataset.as_numpy_iterator())\n    \n    def decode_fn(self, record_bytes):\n        \"\"\"Decode TFRecord data\"\"\"\n        schema = {\n            col: tf.io.VarLenFeature(dtype=tf.float32)\n            for col in FEATURE_COLUMNS\n        }\n        schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n        \n        features = tf.io.parse_single_example(record_bytes, schema)\n        phrase = features[\"phrase\"]\n        landmarks = tf.stack([\n            tf.sparse.to_dense(features[col])\n            for col in FEATURE_COLUMNS\n        ], axis=1)\n        \n        return landmarks, phrase\n    \n    def convert_fn(self, landmarks, phrase):\n        \"\"\"Convert and preprocess the data\"\"\"\n        landmarks = self.preprocess_landmarks(landmarks)\n        phrase = self.tokenizer.encode(phrase.numpy().decode('utf-8'))\n        return landmarks, phrase\n    \n    def preprocess_landmarks(self, landmarks):\n        \"\"\"Preprocess landmarks using competition approach\"\"\"\n        # Detect dominant hand\n        rhand = tf.gather(landmarks, [i for i, col in enumerate(FEATURE_COLUMNS) if 'right_hand' in col], axis=1)\n        lhand = tf.gather(landmarks, [i for i, col in enumerate(FEATURE_COLUMNS) if 'left_hand' in col], axis=1)\n        \n        rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n        lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n        \n        rnans = tf.math.count_nonzero(rnan_idx)\n        lnans = tf.math.count_nonzero(lnan_idx)\n        \n        # Use dominant hand\n        hand = lhand if rnans > lnans else rhand\n        \n        # Normalize\n        mean = tf.reduce_mean(hand, axis=0, keepdims=True)\n        std = tf.reduce_std(hand, axis=0, keepdims=True)\n        std = tf.where(tf.equal(std, 0), tf.ones_like(std), std)\n        hand = (hand - mean) / std\n        \n        # Handle sequence length\n        if tf.shape(hand)[0] > self.max_len:\n            hand = tf.image.resize(hand[None], (self.max_len, tf.shape(hand)[1]))[0]\n        else:\n            pad_len = self.max_len - tf.shape(hand)[0]\n            hand = tf.pad(hand, [[0, pad_len], [0, 0]])\n        \n        return hand\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        landmarks, tokens = self.data[idx]\n        return {\n            'landmarks': torch.from_numpy(landmarks).float(),\n            'tokens': torch.from_numpy(tokens),\n            'phrase': tokens.tobytes().decode('utf-8'),\n            'length': torch.tensor(len(tokens))\n        }\n\ndef collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n    \"\"\"Custom collate function for batching\"\"\"\n    max_token_len = max(item['tokens'].size(0) for item in batch)\n    \n    landmarks = torch.stack([item['landmarks'] for item in batch])\n    tokens = torch.stack([\n        F.pad(item['tokens'], (0, max_token_len - item['tokens'].size(0)), value=59)  # pad_token_idx = 59\n        for item in batch\n    ])\n    lengths = torch.stack([item['length'] for item in batch])\n    \n    return {\n        'landmarks': landmarks,\n        'tokens': tokens,\n        'phrase': [item['phrase'] for item in batch],\n        'length': lengths\n    }\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, input_channels: int = 3, output_dim: int = 52):\n        super().__init__()\n        self.conv = nn.Conv1d(input_channels, 64, kernel_size=3, padding=1)\n        self.bn = nn.BatchNorm1d(64)\n        self.linear = nn.Linear(64, output_dim)\n\n    def forward(self, x):\n        B, T, L, C = x.shape\n        x = x.permute(0, 1, 3, 2)\n        x = x.reshape(B * T, C, L)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = F.relu(x)\n        x = x.mean(dim=2)\n        x = self.linear(x)\n        x = x.reshape(B, T, -1)\n        return x\n\nclass RotaryPositionalEmbedding(nn.Module):\n    def __init__(self, dim: int, max_seq_len: int = 128):  # Changed from 384 to 128\n        super().__init__()\n        head_dim = dim // 8\n        half_head_dim = head_dim // 2\n        emb = math.log(10000) / (half_head_dim - 1)\n        emb = torch.exp(torch.arange(half_head_dim) * -emb)\n        pos = torch.arange(max_seq_len)\n        emb = pos[:, None] * emb[None, :]\n        self.register_buffer('sin', emb.sin())\n        self.register_buffer('cos', emb.cos())\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        return self.sin[:seq_len], self.cos[:seq_len]\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.dim = dim\n        self.head_dim = dim // num_heads\n        assert self.head_dim * num_heads == dim\n        \n        self.q_proj = nn.Linear(dim, dim)\n        self.k_proj = nn.Linear(dim, dim)\n        self.v_proj = nn.Linear(dim, dim)\n        self.out_proj = nn.Linear(dim, dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def apply_rotary_pos_emb(self, q, k, sin, cos):\n        sin = sin.unsqueeze(0).unsqueeze(2)\n        cos = cos.unsqueeze(0).unsqueeze(2)\n        q1, q2 = q.chunk(2, dim=-1)\n        k1, k2 = k.chunk(2, dim=-1)\n        q = torch.cat([q1 * cos - q2 * sin, q2 * cos + q1 * sin], dim=-1)\n        k = torch.cat([k1 * cos - k2 * sin, k2 * cos + k1 * sin], dim=-1)\n        return q, k\n\n    def forward(self, x: torch.Tensor, sin: torch.Tensor, cos: torch.Tensor, \n                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        B, L, D = x.shape\n        \n        q = self.q_proj(x).reshape(B, L, self.num_heads, self.head_dim)\n        k = self.k_proj(x).reshape(B, L, self.num_heads, self.head_dim)\n        v = self.v_proj(x).reshape(B, L, self.num_heads, self.head_dim)\n        \n        q, k = self.apply_rotary_pos_emb(q, k, sin, cos)\n        \n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n        \n        scale = self.head_dim ** -0.5\n        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n        \n        if mask is not None:\n            mask = mask.unsqueeze(1).unsqueeze(2)\n            attn = attn.masked_fill(~mask, float('-inf'))\n        \n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        out = torch.matmul(attn, v)\n        out = out.transpose(1, 2).contiguous()\n        out = out.reshape(B, L, D)\n        \n        return self.out_proj(out)\n\nclass SqueezeformerBlock(nn.Module):\n    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):\n        super().__init__()\n        self.dim = dim\n        self.norm1 = nn.LayerNorm(dim)\n        self.mhsa = MultiHeadAttention(dim, num_heads, dropout)\n        \n        # Feed forward modules\n        self.ff1_norm = nn.LayerNorm(dim)\n        self.ff1 = nn.Sequential(\n            nn.Linear(dim, dim*4),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim*4, dim),\n            nn.Dropout(dropout)\n        )\n        \n        # Convolution module\n        self.conv_norm = nn.LayerNorm(dim)\n        self.conv1 = nn.Conv1d(dim, dim*2, 1)\n        self.glu = nn.GLU(dim=1)\n        self.depthwise_conv = nn.Conv1d(dim, dim, 3, padding=1, groups=dim)\n        self.batch_norm = nn.BatchNorm1d(dim)\n        self.activation = nn.SiLU()\n        self.pointwise_conv = nn.Conv1d(dim, dim, 1)\n        self.conv_dropout = nn.Dropout(dropout)\n        \n        # Feed forward module 2\n        self.ff2_norm = nn.LayerNorm(dim)\n        self.ff2 = nn.Sequential(\n            nn.Linear(dim, dim*4),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim*4, dim),\n            nn.Dropout(dropout)\n        )\n        \n        self.dropout = nn.Dropout(dropout)\n        self.scale = nn.Parameter(torch.ones(1))\n\n    def forward(self, x: torch.Tensor, sin: torch.Tensor, cos: torch.Tensor,\n                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        # First feed forward\n        residual = x\n        x = self.ff1_norm(x)\n        x = self.ff1(x)\n        x = residual + x * self.scale\n        \n        # Self attention\n        residual = x\n        x = self.norm1(x)\n        x = self.mhsa(x, sin, cos, mask)\n        x = self.dropout(x)\n        x = residual + x * self.scale\n        \n        # Convolution module\n        residual = x\n        x = self.conv_norm(x)\n        x = x.transpose(1, 2)\n        x = self.conv1(x)\n        x = self.glu(x)\n        x = self.depthwise_conv(x)\n        x = self.batch_norm(x)\n        x = self.activation(x)\n        x = self.pointwise_conv(x)\n        x = self.conv_dropout(x)\n        x = x.transpose(1, 2)\n        x = residual + x * self.scale\n        \n        # Second feed forward\n        residual = x\n        x = self.ff2_norm(x)\n        x = self.ff2(x)\n        x = residual + x * self.scale\n        \n        return x\n\nclass ASLTranslationModel(nn.Module):\n    def __init__(\n        self,\n        num_landmarks: int = 130,\n        feature_dim: int = 208,\n        num_classes: int = 62,  # Updated to match competition (59 + 3 special tokens)\n        num_layers: int = 2,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        \n        # Feature extractors\n        self.face_extractor = FeatureExtractor(3, 52)\n        self.pose_extractor = FeatureExtractor(3, 52)\n        self.left_hand_extractor = FeatureExtractor(3, 52)\n        self.right_hand_extractor = FeatureExtractor(3, 52)\n        \n        # Embeddings\n        self.target_embedding = nn.Embedding(num_classes, feature_dim)\n        self.pos_embedding = RotaryPositionalEmbedding(feature_dim)\n        \n        # Squeezeformer encoder\n        self.squeezeformer_layers = nn.ModuleList([\n            SqueezeformerBlock(feature_dim, dropout=dropout)\n            for _ in range(num_layers)\n        ])\n        \n        # Decoder\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=feature_dim,\n            nhead=8,\n            dim_feedforward=feature_dim*4,\n            dropout=dropout,\n            batch_first=True,\n            norm_first=True\n        )\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n        \n        # Output layers\n        self.confidence_head = nn.Linear(feature_dim, 1)\n        self.classifier = nn.Linear(feature_dim, num_classes)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n        tgt: Optional[torch.Tensor] = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        B, T, L, C = x.shape\n        \n        # Extract features\n        face = x[:, :, :76]\n        pose = x[:, :, 76:88]\n        left_hand = x[:, :, 88:109]\n        right_hand = x[:, :, 109:]\n        \n        # Process each part\n        face_feats = self.face_extractor(face)\n        pose_feats = self.pose_extractor(pose)\n        left_hand_feats = self.left_hand_extractor(left_hand)\n        right_hand_feats = self.right_hand_extractor(right_hand)\n        \n        # Combine features\n        features = torch.cat([face_feats, pose_feats, left_hand_feats, right_hand_feats], dim=-1)\n        \n        # Get positional embeddings\n        sin, cos = self.pos_embedding(features)\n        \n        # Encoder\n        encoder_out = features\n        encoder_padding_mask = mask if mask is not None else None\n        \n        for layer in self.squeezeformer_layers:\n            encoder_out = layer(encoder_out, sin, cos, encoder_padding_mask)\n        \n        confidence = self.confidence_head(encoder_out[:, 0]).squeeze(-1)\n        \n        if tgt is not None:\n            tgt_embedded = self.target_embedding(tgt)\n            tgt_embedded = self.dropout(tgt_embedded)\n            \n            tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(x.device)\n            memory_padding_mask = ~encoder_padding_mask if encoder_padding_mask is not None else None\n            \n            decoder_out = self.decoder(\n                tgt_embedded,\n                encoder_out,\n                tgt_mask=tgt_mask,\n                memory_key_padding_mask=memory_padding_mask\n            )\n            output = self.classifier(decoder_out)\n        else:\n            output = self.classifier(encoder_out)\n        \n        return output, confidence\n\n    @staticmethod\n    def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n        mask = mask.masked_fill(mask == 1, float('-inf'))\n        mask = mask.masked_fill(mask == 0, float(0.0))\n        return mask\n\nclass ASLTranslationLoss(nn.Module):\n    def __init__(self, pad_idx: int = 59):  # 59 is pad token index\n        super().__init__()\n        self.criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n        \n    def forward(\n        self,\n        pred: torch.Tensor,\n        target: torch.Tensor,\n        confidence: torch.Tensor,\n        confidence_target: torch.Tensor\n    ) -> torch.Tensor:\n        seq_loss = self.criterion(pred.reshape(-1, pred.size(-1)), target.reshape(-1))\n        conf_loss = F.mse_loss(confidence, confidence_target)\n        return seq_loss + 0.1 * conf_loss\n\n\nclass Trainer:\n    def __init__(\n        self,\n        model: nn.Module,\n        train_loader: DataLoader,\n        val_loader: DataLoader,\n        tokenizer: ASLTokenizer,\n        learning_rate: float = 0.0001,  # Changed to match competition\n        weight_decay: float = 0.08,\n        warmup_epochs: int = 1,\n        max_epochs: int = 13,  # Changed to match competition\n        device: str = 'cuda',\n        wandb_config: dict = None\n    ):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.tokenizer = tokenizer\n        self.device = device\n        self.max_epochs = max_epochs\n        self.wandb_config = wandb_config\n        \n        if wandb_config:\n            wandb.init(\n                project=wandb_config['asl-translation'],\n                name=wandb_config['run_name'],\n                config={\n                    'learning_rate': learning_rate,\n                    'weight_decay': weight_decay,\n                    'warmup_epochs': warmup_epochs,\n                    'max_epochs': max_epochs,\n                    'batch_size': train_loader.batch_size,\n                    'architecture': 'Squeezeformer'\n                }\n            )\n            wandb.watch(model, log_freq=100)\n        \n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=learning_rate,\n            weight_decay=weight_decay\n        )\n        \n        self.num_training_steps = len(train_loader) * max_epochs\n        self.num_warmup_steps = len(train_loader) * warmup_epochs\n        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            self.optimizer,\n            max_lr=learning_rate,\n            total_steps=self.num_training_steps,\n            pct_start=self.num_warmup_steps / self.num_training_steps,\n            anneal_strategy='cos',\n            cycle_momentum=False\n        )\n        \n        self.criterion = ASLTranslationLoss()\n        self.scaler = GradScaler()\n    \n    def train_epoch(self) -> float:\n        self.model.train()\n        total_loss = 0\n        \n        progress_bar = tqdm(\n            self.train_loader,\n            desc='Training',\n            leave=True,\n            dynamic_ncols=True\n        )\n        \n        for batch_idx, batch in enumerate(progress_bar):\n            self.optimizer.zero_grad()\n            \n            landmarks = batch['landmarks'].to(self.device)\n            tokens = batch['tokens'].to(self.device)\n            lengths = batch['length'].to(self.device)\n            \n            # Create mask\n            mask = torch.arange(landmarks.size(1), device=self.device)[None, :] < lengths[:, None]\n            \n            with autocast():\n                pred, confidence = self.model(landmarks, mask, tokens[:, :-1])\n                \n                with torch.no_grad():\n                    confidence_target = torch.tensor([\n                        1 - Levenshtein.distance(\n                            self.tokenizer.decode(p.argmax(-1).cpu()),\n                            true_text\n                        ) / max(len(true_text), 1)\n                        for p, true_text in zip(pred, batch['phrase'])\n                    ], device=self.device)\n                \n                loss = self.criterion(pred, tokens[:, 1:], confidence, confidence_target)\n            \n            self.scaler.scale(loss).backward()\n            self.scaler.unscale_(self.optimizer)\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            self.scheduler.step()\n            \n            total_loss += loss.item()\n            current_lr = self.optimizer.param_groups[0]['lr']\n            \n            if self.wandb_config and batch_idx % 10 == 0:\n                wandb.log({\n                    'batch_loss': loss.item(),\n                    'learning_rate': current_lr,\n                    'batch_confidence': confidence.mean().item(),\n                    'batch_confidence_target': confidence_target.mean().item()\n                })\n            \n            progress_bar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'lr': f'{current_lr:.2e}'\n            })\n        \n        return total_loss / len(self.train_loader)\n    \n    @torch.no_grad()\n    def validate(self) -> Tuple[float, float]:\n        self.model.eval()\n        total_loss = 0\n        predictions = []\n        ground_truth = []\n        confidence_scores = []\n        \n        for batch in tqdm(self.val_loader, desc='Validating'):\n            landmarks = batch['landmarks'].to(self.device)\n            tokens = batch['tokens'].to(self.device)\n            lengths = batch['length'].to(self.device)\n            \n            mask = torch.arange(landmarks.size(1), device=self.device)[None, :] < lengths[:, None]\n            \n            pred, confidence = self.model(landmarks, mask)\n            confidence_scores.extend(confidence.cpu().tolist())\n            \n            pred_texts = [self.tokenizer.decode(p.argmax(-1)) for p in pred]\n            predictions.extend(pred_texts)\n            ground_truth.extend(batch['phrase'])\n            \n            confidence_target = torch.tensor([\n                1 - Levenshtein.distance(pred_text, true_text) / max(len(true_text), 1)\n                for pred_text, true_text in zip(pred_texts, batch['phrase'])\n            ]).to(self.device)\n            \n            loss = self.criterion(pred, tokens[:, 1:], confidence, confidence_target)\n            total_loss += loss.item()\n        \n        avg_loss = total_loss / len(self.val_loader)\n        distances = [\n            1 - Levenshtein.distance(pred, true) / max(len(pred), len(true))\n            for pred, true in zip(predictions, ground_truth)\n        ]\n        avg_score = sum(distances) / len(distances)\n        \n        if self.wandb_config:\n            wandb.log({\n                'val_loss': avg_loss,\n                'val_score': avg_score,\n                'val_confidence_mean': np.mean(confidence_scores),\n                'val_confidence_std': np.std(confidence_scores)\n            })\n        \n        return avg_loss, avg_score\n    \n    def train(self, save_dir: str):\n        os.makedirs(save_dir, exist_ok=True)\n        best_val_score = float('-inf')\n        \n        for epoch in range(self.max_epochs):\n            print(f\"\\nEpoch {epoch + 1}/{self.max_epochs}\")\n            train_loss = self.train_epoch()\n            \n            if (epoch + 1) % 2 == 0:  # Validate every 2 epochs\n                val_loss, val_score = self.validate()\n                print(f\"Validation Loss: {val_loss:.4f}\")\n                print(f\"Validation Score: {val_score:.4f}\")\n                \n                if val_score > best_val_score:\n                    best_val_score = val_score\n                    torch.save({\n                        'epoch': epoch,\n                        'model_state_dict': self.model.state_dict(),\n                        'optimizer_state_dict': self.optimizer.state_dict(),\n                        'scheduler_state_dict': self.scheduler.state_dict(),\n                        'val_score': val_score,\n                    }, os.path.join(save_dir, 'best_model.pt'))\n            \n            if (epoch + 1) % 5 == 0:  # Save checkpoint every 5 epochs\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'scheduler_state_dict': self.scheduler.state_dict(),\n                }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pt'))\n\ndef create_dataloaders(\n    processed_dir: str,\n    tokenizer,\n    batch_size: int = 64,\n    num_workers: int = 4\n):\n    \"\"\"Create train and validation dataloaders\"\"\"\n    # Get TFRecord files\n    tf_records = sorted(list(Path(processed_dir).glob('chunk_*.tfrecord')))\n    train_size = int(len(tf_records) * 0.8)\n    \n    # Create datasets\n    train_dataset = ASLDataset(\n        tf_records[:train_size],\n        tokenizer,\n        augment=True,\n        mode='train'\n    )\n    \n    val_dataset = ASLDataset(\n        tf_records[train_size:],\n        tokenizer,\n        augment=False,\n        mode='val'\n    )\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        persistent_workers=True,\n        collate_fn=collate_fn\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n        persistent_workers=True,\n        collate_fn=collate_fn\n    )\n    \n    return train_loader, val_loader\n\ndef main():\n    config = {\n        'data_dir': '/kaggle/input/asl-fingerspelling/train_landmarks',\n        'metadata_path': '/kaggle/input/asl-fingerspelling/train.csv',\n        'vocab_path': '/kaggle/input/asl-fingerspelling/character_to_prediction_index.json',\n        'processed_dir': '/kaggle/working/processed_data',\n        'save_dir': '/kaggle/working/models',\n        'batch_size': 64,\n        'num_workers': 4,\n        'learning_rate': 0.0001,  # Competition learning rate\n        'weight_decay': 0.08,\n        'warmup_epochs': 1,\n        'max_epochs': 13,  # Competition number of epochs\n        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n    }\n    \n    # Set random seeds\n    torch.manual_seed(42)\n    random.seed(42)\n    np.random.seed(42)\n    \n    # Preprocess data if not already done\n    if not os.path.exists(os.path.join(config['processed_dir'], 'metadata.json')):\n        print(\"Preprocessing data...\")\n        preprocess_data(\n            config['data_dir'],\n            config['metadata_path'],\n            config['processed_dir']\n        )\n    \n    # Initialize tokenizer\n    tokenizer = ASLTokenizer(config['vocab_path'])\n    \n    # Create dataloaders\n    train_loader, val_loader = create_dataloaders(\n        config['processed_dir'],\n        tokenizer,\n        batch_size=config['batch_size'],\n        num_workers=config['num_workers']\n    )\n    \n    # Initialize model\n    model = ASLTranslationModel(\n        num_landmarks=130,\n        feature_dim=208,\n        num_classes=62,  # 59 chars + 3 special tokens\n        num_layers=2,\n        dropout=0.1\n    )\n    \n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n    \n    # Initialize trainer\n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        tokenizer=tokenizer,\n        learning_rate=config['learning_rate'],\n        weight_decay=config['weight_decay'],\n        warmup_epochs=config['warmup_epochs'],\n        max_epochs=config['max_epochs'],\n        device=config['device'],\n        wandb_config={\n            'project': 'asl-translation',\n            'run_name': f'asl-translation-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n        }\n    )\n    \n    # Train model\n    print(\"\\nStarting training...\")\n    trainer.train(config['save_dir'])\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:53:13.924161Z","iopub.execute_input":"2024-11-29T08:53:13.924471Z"}},"outputs":[],"execution_count":null}]}