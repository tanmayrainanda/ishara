{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a451cfd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:05:39.512472Z",
     "iopub.status.busy": "2024-12-08T20:05:39.512166Z",
     "iopub.status.idle": "2024-12-08T20:05:47.068637Z",
     "shell.execute_reply": "2024-12-08T20:05:47.067866Z"
    },
    "papermill": {
     "duration": 7.564008,
     "end_time": "2024-12-08T20:05:47.070517",
     "exception": false,
     "start_time": "2024-12-08T20:05:39.506509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf49e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:05:47.080313Z",
     "iopub.status.busy": "2024-12-08T20:05:47.079778Z",
     "iopub.status.idle": "2024-12-08T20:05:47.261611Z",
     "shell.execute_reply": "2024-12-08T20:05:47.260942Z"
    },
    "papermill": {
     "duration": 0.188756,
     "end_time": "2024-12-08T20:05:47.263573",
     "exception": false,
     "start_time": "2024-12-08T20:05:47.074817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    char_to_num = json.load(f)\n",
    "\n",
    "pad_token = '^'\n",
    "pad_token_idx = 59\n",
    "\n",
    "char_to_num[pad_token] = pad_token_idx\n",
    "\n",
    "num_to_char = {j:i for i,j in char_to_num.items()}\n",
    "df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n",
    "\n",
    "LIP = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE\n",
    "\n",
    "X = [f'x_right_hand_{i}' for i in range(21)]  +[f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE] + [f'x_face_{i}' for i in LIP] #+ \n",
    "Y = [f'y_right_hand_{i}' for i in range(21)]  +[f'y_left_hand_{i}' for i in range(21)]+ [f'y_pose_{i}' for i in POSE] + [f'y_face_{i}' for i in LIP] #+\n",
    "Z = [f'z_right_hand_{i}' for i in range(21)]  + [f'z_left_hand_{i}' for i in range(21)]+ [f'z_pose_{i}' for i in POSE] + [f'z_face_{i}' for i in LIP] #+ \n",
    "\n",
    "SEL_COLS = X + Y + Z\n",
    "FRAME_LEN = 128 + 48\n",
    "MAX_PHRASE_LENGTH = 64\n",
    "\n",
    "LIP_IDX_X   = [i for i, col in enumerate(SEL_COLS)  if  \"face\" in col and \"x\" in col]\n",
    "RHAND_IDX_X = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col and \"x\" in col]\n",
    "LHAND_IDX_X = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col and \"x\" in col]\n",
    "RPOSE_IDX_X = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE and \"x\" in col]\n",
    "LPOSE_IDX_X = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE and \"x\" in col]\n",
    "\n",
    "LIP_IDX_Y   = [i for i, col in enumerate(SEL_COLS)  if  \"face\" in col and \"y\" in col]\n",
    "RHAND_IDX_Y = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col and \"y\" in col]\n",
    "LHAND_IDX_Y = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col and \"y\" in col]\n",
    "RPOSE_IDX_Y = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE and \"y\" in col]\n",
    "LPOSE_IDX_Y = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE and \"y\" in col]\n",
    "\n",
    "LIP_IDX_Z   = [i for i, col in enumerate(SEL_COLS)  if  \"face\" in col and \"z\" in col]\n",
    "RHAND_IDX_Z = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col and \"z\" in col]\n",
    "LHAND_IDX_Z = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col and \"z\" in col]\n",
    "RPOSE_IDX_Z = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE and \"z\" in col]\n",
    "LPOSE_IDX_Z = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE and \"z\" in col]\n",
    "\n",
    "RHM = np.load(\"/kaggle/input/aslfr-dataset-tfrecords/mean_std/rh_mean.npy\")\n",
    "LHM = np.load(\"/kaggle/input/aslfr-dataset-tfrecords/mean_std/lh_mean.npy\")\n",
    "RPM = np.load(\"/kaggle/input/aslfr-dataset-tfrecords/mean_std/rp_mean.npy\")\n",
    "LPM = np.load(\"/kaggle/input/aslfr-dataset-tfrecords/mean_std/lp_mean.npy\")\n",
    "LIPM = np.load(\"/kaggle/input/aslfr-dataset-tfrecords/mean_std/lip_mean.npy\")\n",
    "\n",
    "RHS = np.load(\"/kaggle/input/aslfr-dataset-tfrecords/mean_std/rh_std.npy\")\n",
    "LHS = np.load(\"/kaggle/input/aslfr-dataset-tfrecords/mean_std/lh_std.npy\")\n",
    "RPS = np.load(\"/kaggle/input/aslfr-dataset-tfrecords/mean_std/rp_std.npy\")\n",
    "LPS = np.load(\"/kaggle/input/aslfr-dataset-tfrecords/mean_std/lp_std.npy\")\n",
    "LIPS = np.load(\"/kaggle/input/aslfr-dataset-tfrecords/mean_std/lip_std.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9b20c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:05:47.272920Z",
     "iopub.status.busy": "2024-12-08T20:05:47.272681Z",
     "iopub.status.idle": "2024-12-08T20:05:51.061051Z",
     "shell.execute_reply": "2024-12-08T20:05:51.060359Z"
    },
    "papermill": {
     "duration": 3.795118,
     "end_time": "2024-12-08T20:05:51.062983",
     "exception": false,
     "start_time": "2024-12-08T20:05:47.267865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_relevant_data_subset(pq_path):\n",
    "    return pd.read_parquet(pq_path, columns=SEL_COLS)\n",
    "\n",
    "file_id = df.file_id.iloc[0]\n",
    "inpdir = \"/kaggle/input/asl-fingerspelling/train_landmarks\"\n",
    "pqfile = f\"{inpdir}/{file_id}.parquet\"\n",
    "seq_refs = df.loc[df.file_id == file_id]\n",
    "seqs = load_relevant_data_subset(pqfile)\n",
    "\n",
    "seq_id = seq_refs.sequence_id.iloc[0]\n",
    "frames = seqs.iloc[seqs.index == seq_id]\n",
    "phrase = str(df.loc[df.sequence_id == seq_id].phrase.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5d6ec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:05:51.072378Z",
     "iopub.status.busy": "2024-12-08T20:05:51.072108Z",
     "iopub.status.idle": "2024-12-08T20:05:54.430296Z",
     "shell.execute_reply": "2024-12-08T20:05:54.428841Z"
    },
    "papermill": {
     "duration": 3.364777,
     "end_time": "2024-12-08T20:05:54.431981",
     "exception": true,
     "start_time": "2024-12-08T20:05:51.067204",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_26/237357032.py\", line 106, in pre_process1  *\n        rhand = (resize_pad(rhand) - RHM) / RHS\n\n    NotImplementedError: Cannot convert a symbolic tf.Tensor (PartitionedCall_1:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    117\u001b[0m pre0 \u001b[38;5;241m=\u001b[39m pre_process0(frames)\n\u001b[0;32m--> 118\u001b[0m pre1 \u001b[38;5;241m=\u001b[39m \u001b[43mpre_process1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpre0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m INPUT_SHAPE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(pre1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(INPUT_SHAPE)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileutmu7gv2.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__pre_process1\u001b[0;34m(lip, rhand, lhand, rpose, lpose)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m lip \u001b[38;5;241m=\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(resize_pad), (ag__\u001b[38;5;241m.\u001b[39mld(lip),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(LIPM)) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(LIPS)\n\u001b[0;32m---> 11\u001b[0m rhand \u001b[38;5;241m=\u001b[39m (\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresize_pad\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhand\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRHM\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(RHS)\n\u001b[1;32m     12\u001b[0m lhand \u001b[38;5;241m=\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(resize_pad), (ag__\u001b[38;5;241m.\u001b[39mld(lhand),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(LHM)) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(LHS)\n\u001b[1;32m     13\u001b[0m rpose \u001b[38;5;241m=\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(resize_pad), (ag__\u001b[38;5;241m.\u001b[39mld(rpose),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(RPM)) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(RPS)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_26/237357032.py\", line 106, in pre_process1  *\n        rhand = (resize_pad(rhand) - RHM) / RHS\n\n    NotImplementedError: Cannot convert a symbolic tf.Tensor (PartitionedCall_1:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n"
     ]
    }
   ],
   "source": [
    "@tf.function()\n",
    "def resize_pad(x):\n",
    "    if tf.shape(x)[0] < FRAME_LEN:\n",
    "        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]), constant_values=float(\"NaN\"))\n",
    "    else:\n",
    "        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n",
    "    return x\n",
    "\n",
    "#@tf.function()\n",
    "def pre_process0(x):\n",
    "    lip_x = tf.gather(x, LIP_IDX_X, axis=1)\n",
    "    lip_y = tf.gather(x, LIP_IDX_Y, axis=1)\n",
    "    lip_z = tf.gather(x, LIP_IDX_Z, axis=1)\n",
    "\n",
    "    rhand_x = tf.gather(x, RHAND_IDX_X, axis=1)\n",
    "    rhand_y = tf.gather(x, RHAND_IDX_Y, axis=1)\n",
    "    rhand_z = tf.gather(x, RHAND_IDX_Z, axis=1)\n",
    "    \n",
    "    lhand_x = tf.gather(x, LHAND_IDX_X, axis=1)\n",
    "    lhand_y = tf.gather(x, LHAND_IDX_Y, axis=1)\n",
    "    lhand_z = tf.gather(x, LHAND_IDX_Z, axis=1)\n",
    "\n",
    "    rpose_x = tf.gather(x, RPOSE_IDX_X, axis=1)\n",
    "    rpose_y = tf.gather(x, RPOSE_IDX_Y, axis=1)\n",
    "    rpose_z = tf.gather(x, RPOSE_IDX_Z, axis=1)\n",
    "    \n",
    "    lpose_x = tf.gather(x, LPOSE_IDX_X, axis=1)\n",
    "    lpose_y = tf.gather(x, LPOSE_IDX_Y, axis=1)\n",
    "    lpose_z = tf.gather(x, LPOSE_IDX_Z, axis=1)\n",
    "    \n",
    "    lip   = tf.concat([lip_x[..., tf.newaxis], lip_y[..., tf.newaxis], lip_z[..., tf.newaxis]], axis=-1)\n",
    "    rhand = tf.concat([rhand_x[..., tf.newaxis], rhand_y[..., tf.newaxis], rhand_z[..., tf.newaxis]], axis=-1)\n",
    "    lhand = tf.concat([lhand_x[..., tf.newaxis], lhand_y[..., tf.newaxis], lhand_z[..., tf.newaxis]], axis=-1)\n",
    "    rpose = tf.concat([rpose_x[..., tf.newaxis], rpose_y[..., tf.newaxis], rpose_z[..., tf.newaxis]], axis=-1)\n",
    "    lpose = tf.concat([lpose_x[..., tf.newaxis], lpose_y[..., tf.newaxis], lpose_z[..., tf.newaxis]], axis=-1)\n",
    "        \n",
    "    # TIME AUGMENTATION\n",
    "    if tf.random.uniform(shape=(), minval=0, maxval=1) < 0.2:\n",
    "        new_width = tf.shape(lip)[1]\n",
    "        height = tf.cast(tf.shape(lip)[0], tf.float32)\n",
    "        min_height = tf.cast(height / 2.0, tf.int32)\n",
    "        max_height = tf.cast(height * 1.5, tf.int32)\n",
    "        \n",
    "        new_height = tf.random.uniform(\n",
    "            shape=(), \n",
    "            minval=min_height,\n",
    "            maxval=max_height,\n",
    "            dtype=tf.int32\n",
    "        )\n",
    "        \n",
    "        resized_lip = tf.image.resize(lip, (new_height, new_width))\n",
    "        resized_rhand = tf.image.resize(rhand, (new_height, new_width))\n",
    "        resized_lhand = tf.image.resize(lhand, (new_height, new_width))\n",
    "        resized_rpose = tf.image.resize(rpose, (new_height, new_width))\n",
    "        resized_lpose = tf.image.resize(lpose, (new_height, new_width))\n",
    "        \n",
    "        return resized_lip, resized_rhand, resized_lhand, resized_rpose, resized_lpose\n",
    "        \n",
    "    return lip, rhand, lhand, rpose, lpose\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def pre_process00(x):\n",
    "    lip_x = tf.gather(x, LIP_IDX_X, axis=1)\n",
    "    lip_y = tf.gather(x, LIP_IDX_Y, axis=1)\n",
    "    lip_z = tf.gather(x, LIP_IDX_Z, axis=1)\n",
    "\n",
    "    rhand_x = tf.gather(x, RHAND_IDX_X, axis=1)\n",
    "    rhand_y = tf.gather(x, RHAND_IDX_Y, axis=1)\n",
    "    rhand_z = tf.gather(x, RHAND_IDX_Z, axis=1)\n",
    "    \n",
    "    lhand_x = tf.gather(x, LHAND_IDX_X, axis=1)\n",
    "    lhand_y = tf.gather(x, LHAND_IDX_Y, axis=1)\n",
    "    lhand_z = tf.gather(x, LHAND_IDX_Z, axis=1)\n",
    "\n",
    "    rpose_x = tf.gather(x, RPOSE_IDX_X, axis=1)\n",
    "    rpose_y = tf.gather(x, RPOSE_IDX_Y, axis=1)\n",
    "    rpose_z = tf.gather(x, RPOSE_IDX_Z, axis=1)\n",
    "    \n",
    "    lpose_x = tf.gather(x, LPOSE_IDX_X, axis=1)\n",
    "    lpose_y = tf.gather(x, LPOSE_IDX_Y, axis=1)\n",
    "    lpose_z = tf.gather(x, LPOSE_IDX_Z, axis=1)\n",
    "    \n",
    "    lip   = tf.concat([lip_x[..., tf.newaxis], lip_y[..., tf.newaxis], lip_z[..., tf.newaxis]], axis=-1)\n",
    "    rhand = tf.concat([rhand_x[..., tf.newaxis], rhand_y[..., tf.newaxis], rhand_z[..., tf.newaxis]], axis=-1)\n",
    "    lhand = tf.concat([lhand_x[..., tf.newaxis], lhand_y[..., tf.newaxis], lhand_z[..., tf.newaxis]], axis=-1)\n",
    "    rpose = tf.concat([rpose_x[..., tf.newaxis], rpose_y[..., tf.newaxis], rpose_z[..., tf.newaxis]], axis=-1)\n",
    "    lpose = tf.concat([lpose_x[..., tf.newaxis], lpose_y[..., tf.newaxis], lpose_z[..., tf.newaxis]], axis=-1)\n",
    "                \n",
    "    hand = tf.concat([rhand, lhand], axis=1)\n",
    "    hand = tf.where(tf.math.is_nan(hand), 0.0, hand)\n",
    "    input_tensor = tf.math.not_equal(tf.reduce_sum(hand, axis=[1, 2]), 0.0)\n",
    "    alternating_tensor = tf.math.equal( tf.cumsum(tf.ones_like( tf.reduce_sum(hand, axis=[1, 2]) ))%2, 1.0 )\n",
    "    mask = tf.math.logical_or(input_tensor, alternating_tensor)\n",
    "    \n",
    "    lip = lip[mask]\n",
    "    rhand = rhand[mask]\n",
    "    lhand = lhand[mask]\n",
    "    rpose = rpose[mask]\n",
    "    lpose = lpose[mask]\n",
    "\n",
    "    return lip, rhand, lhand,  rpose, lpose\n",
    "\n",
    "@tf.function()\n",
    "def pre_process1(lip, rhand,lhand,  rpose, lpose): #lhand,\n",
    "    lip   = (resize_pad(lip) - LIPM) / LIPS\n",
    "    rhand = (resize_pad(rhand) - RHM) / RHS\n",
    "    lhand = (resize_pad(lhand) - LHM) / LHS\n",
    "    rpose = (resize_pad(rpose) - RPM) / RPS\n",
    "    lpose = (resize_pad(lpose) - LPM) / LPS\n",
    "\n",
    "    x = tf.concat([lip, rhand, lhand, rpose, lpose], axis=1) #lhand,\n",
    "    s = tf.shape(x)\n",
    "    x = tf.reshape(x, (s[0], s[1]*s[2]))\n",
    "    x = tf.where(tf.math.is_nan(x), 0.0, x)\n",
    "    return x\n",
    "\n",
    "pre0 = pre_process0(frames)\n",
    "pre1 = pre_process1(*pre0)\n",
    "INPUT_SHAPE = list(pre1.shape)\n",
    "print(INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093021c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T15:16:49.747337Z",
     "iopub.status.busy": "2023-07-28T15:16:49.746934Z",
     "iopub.status.idle": "2023-07-28T15:16:50.454515Z",
     "shell.execute_reply": "2023-07-28T15:16:50.453395Z",
     "shell.execute_reply.started": "2023-07-28T15:16:49.7473Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_fn(record_bytes):\n",
    "    schema = {\n",
    "        \"lip\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"rhand\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"lhand\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"rpose\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"lpose\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"phrase\": tf.io.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    x = tf.io.parse_single_example(record_bytes, schema)\n",
    "\n",
    "    lip = tf.reshape(tf.sparse.to_dense(x[\"lip\"]), (-1, 40, 3))\n",
    "    rhand = tf.reshape(tf.sparse.to_dense(x[\"rhand\"]), (-1, 21, 3))\n",
    "    lhand = tf.reshape(tf.sparse.to_dense(x[\"lhand\"]), (-1, 21, 3))\n",
    "    rpose = tf.reshape(tf.sparse.to_dense(x[\"rpose\"]), (-1, 5, 3))\n",
    "    lpose = tf.reshape(tf.sparse.to_dense(x[\"lpose\"]), (-1, 5, 3))\n",
    "    phrase = tf.sparse.to_dense(x[\"phrase\"])\n",
    "\n",
    "    return lip, rhand, lhand,  rpose, lpose, phrase #lhand,\n",
    "\n",
    "'''\n",
    "def pre_process_fn(lip, rhand,lhand,  rpose, lpose, phrase): #lhand,\n",
    "    phrase = tf.pad(phrase, [[0, MAX_PHRASE_LENGTH-tf.shape(phrase)[0]]], constant_values=pad_token_idx)\n",
    "    return pre_process1(lip, rhand, lhand, rpose, lpose), phrase #lhand,\n",
    "'''\n",
    "def pre_process_fn(lip, rhand, lhand, rpose, lpose, phrase):\n",
    "    # Preprocess the encoder inputs (landmark features)\n",
    "    encoder_input = pre_process1(lip, rhand, lhand, rpose, lpose)\n",
    "    \n",
    "    # Pad the phrase to a fixed length (MAX_PHRASE_LENGTH) before shifting it\n",
    "    max_length = MAX_PHRASE_LENGTH  # Set to a fixed value\n",
    "    phrase = tf.pad(phrase, [[0, max_length - tf.shape(phrase)[0]]], constant_values=pad_token_idx)\n",
    "    \n",
    "    # The decoder input is the ground-truth phrase shifted by 1 position for teacher forcing\n",
    "    decoder_input = tf.pad(phrase[:-1], [[1, 0]], constant_values=pad_token_idx)  # Shifted sequence for teacher forcing\n",
    "    \n",
    "    # The output is the ground-truth phrase\n",
    "    output = phrase\n",
    "    \n",
    "    return (encoder_input, decoder_input), output\n",
    "\n",
    "    \n",
    "tffiles = [f\"/kaggle/input/chris-tf-v9/{file_id}.tfrecord\" for file_id in df.file_id.unique()]\n",
    "val_len = 1\n",
    "train_batch_size = 64\n",
    "val_batch_size = 64\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(tffiles[val_len:]) \\\n",
    "    .map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "    .map(pre_process_fn, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "    .padded_batch(train_batch_size, padded_shapes=(\n",
    "        ([None, len(SEL_COLS)], [None]),  # Encoder input, Decoder input\n",
    "        [None]  # Output phrase\n",
    "    )) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = tf.data.TFRecordDataset(tffiles[:val_len]).prefetch(tf.data.AUTOTUNE)\\\n",
    "    .map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "    .map(pre_process_fn, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "    .batch(val_batch_size)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "batch = next(iter(val_dataset))\n",
    "print(\"Encoder input shape:\", batch[0][0].shape)  # Encoder input\n",
    "print(\"Decoder input shape:\", batch[0][1].shape)  # Decoder input\n",
    "print(\"Output shape:\", batch[1].shape)  # Ground-truth phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed785c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T15:16:50.456569Z",
     "iopub.status.busy": "2023-07-28T15:16:50.456222Z",
     "iopub.status.idle": "2023-07-28T15:16:50.494542Z",
     "shell.execute_reply": "2023-07-28T15:16:50.493425Z",
     "shell.execute_reply.started": "2023-07-28T15:16:50.456543Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ECA(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
    "        nn = tf.expand_dims(nn, -1)\n",
    "        nn = self.conv(nn)\n",
    "        nn = tf.squeeze(nn, -1)\n",
    "        nn = tf.nn.sigmoid(nn)\n",
    "        nn = nn[:,None,:]\n",
    "        return inputs * nn\n",
    "\n",
    "class CausalDWConv1D(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "        kernel_size=17,\n",
    "        dilation_rate=1,\n",
    "        use_bias=False,\n",
    "        depthwise_initializer='glorot_uniform',\n",
    "        name='', **kwargs):\n",
    "        super().__init__(name=name,**kwargs)\n",
    "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
    "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
    "                            kernel_size,\n",
    "                            strides=1,\n",
    "                            dilation_rate=dilation_rate,\n",
    "                            padding='valid',\n",
    "                            use_bias=use_bias,\n",
    "                            depthwise_initializer=depthwise_initializer,\n",
    "                            name=name + '_dwconv')\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.causal_pad(inputs)\n",
    "        x = self.dw_conv(x)\n",
    "        return x\n",
    "\n",
    "def Conv1DBlock(channel_size,\n",
    "          kernel_size,\n",
    "          dilation_rate=1,\n",
    "          drop_rate=0.0,\n",
    "          expand_ratio=2,\n",
    "          se_ratio=0.25,\n",
    "          activation='swish',\n",
    "          name=None):\n",
    "    '''\n",
    "    efficient conv1d block, @hoyso48\n",
    "    '''\n",
    "    if name is None:\n",
    "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
    "    # Expansion phase\n",
    "    def apply(inputs):\n",
    "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
    "        channels_expand = channels_in * expand_ratio\n",
    "\n",
    "        skip = inputs\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channels_expand,\n",
    "            use_bias=True,\n",
    "            activation=activation,\n",
    "            name=name + '_expand_conv')(inputs)\n",
    "\n",
    "        # Depthwise Convolution\n",
    "        x = CausalDWConv1D(kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            use_bias=False,\n",
    "            name=name + '_dwconv')(x)\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n",
    "\n",
    "        x  = ECA()(x)\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channel_size,\n",
    "            use_bias=True,\n",
    "            name=name + '_project_conv')(x)\n",
    "\n",
    "        if drop_rate > 0:\n",
    "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n",
    "\n",
    "        if (channels_in == channel_size):\n",
    "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.scale = self.dim ** -0.5\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
    "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        qkv = self.qkv(inputs)\n",
    "        \n",
    "        # Reshape into [batch_size, time_steps, num_heads, dim_per_head]\n",
    "        qkv = tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv)\n",
    "        \n",
    "        # Split Q, K, V\n",
    "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
    "\n",
    "        # Attention\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "        \n",
    "        # Apply mask if exists\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :]  # Expand dims for broadcasting\n",
    "            attn += (1.0 - tf.cast(mask, tf.float32)) * -1e9\n",
    "        \n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.drop1(attn)\n",
    "        \n",
    "        # Attention weighted sum\n",
    "        x = tf.matmul(attn, v)\n",
    "        \n",
    "        # ** Fix Here **\n",
    "        # Reshape and permute the tensor correctly\n",
    "        x = tf.reshape(x, (-1, x.shape[1], x.shape[2] * x.shape[3]))  # Flatten (batch_size, time_steps, dim)\n",
    "        \n",
    "        # Projection\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SqueezeExcite(tf.keras.layers.Layer):\n",
    "    def __init__(self, channels, reduction_ratio=8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.gap = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        reduced_channels = max(1, channels // reduction_ratio)\n",
    "        self.fc1 = tf.keras.layers.Dense(reduced_channels, activation='swish')\n",
    "        self.fc2 = tf.keras.layers.Dense(channels, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        x = self.gap(inputs, mask=mask)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return inputs * tf.expand_dims(x, 1)\n",
    "\n",
    "class ConvModule(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, kernel_size, expansion_factor=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.conv1 = tf.keras.layers.Conv1D(dim * expansion_factor, 1)\n",
    "        self.conv2 = CausalDWConv1D(kernel_size=kernel_size)\n",
    "        self.conv3 = tf.keras.layers.Conv1D(dim, 1)\n",
    "        self.se = SqueezeExcite(dim)\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        x = self.norm(inputs)\n",
    "        x = self.conv1(x)\n",
    "        x = tf.keras.activations.swish(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.keras.activations.swish(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.se(x, mask=mask)\n",
    "        return x + inputs\n",
    "\n",
    "class SqueezeformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, expansion_factor=4, kernel_size=31, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        \n",
    "        # Feed Forward Module 1\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn1 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim * expansion_factor, activation='swish'),\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "            tf.keras.layers.Dense(dim)\n",
    "        ])\n",
    "        \n",
    "        # Multi-head Self Attention\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mha = MultiHeadSelfAttention(dim=dim, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        # Convolution Module\n",
    "        self.conv = ConvModule(dim, kernel_size, expansion_factor)\n",
    "        \n",
    "        # Feed Forward Module 2\n",
    "        self.norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn2 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim * expansion_factor, activation='swish'),\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "            tf.keras.layers.Dense(dim)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        # First FFN\n",
    "        residual = inputs\n",
    "        x = self.norm1(inputs)\n",
    "        x = self.ffn1(x)\n",
    "        x = residual + self.dropout(x)\n",
    "        \n",
    "        # Self Attention\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mha(x, mask=mask)\n",
    "        x = residual + self.dropout(x)\n",
    "        \n",
    "        # Convolution\n",
    "        x = self.conv(x, mask=mask)\n",
    "        \n",
    "        # Second FFN\n",
    "        residual = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.ffn2(x)\n",
    "        x = residual + self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "def TransformerBlock(dim=256, num_heads=6, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n",
    "    def apply(inputs):\n",
    "        x = inputs\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([inputs, x])\n",
    "        attn_out = x\n",
    "\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n",
    "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([attn_out, x])\n",
    "        return x\n",
    "    return apply\n",
    "\n",
    "def positional_encoding(maxlen, num_hid):\n",
    "        depth = num_hid/2\n",
    "        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n",
    "        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n",
    "        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n",
    "        angle_rads = tf.linalg.matmul(positions, angle_rates)\n",
    "        pos_encoding = tf.concat(\n",
    "          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n",
    "          axis=-1)\n",
    "        return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6ee39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T15:16:50.496827Z",
     "iopub.status.busy": "2023-07-28T15:16:50.496402Z",
     "iopub.status.idle": "2023-07-28T15:16:50.504277Z",
     "shell.execute_reply": "2023-07-28T15:16:50.503021Z",
     "shell.execute_reply.started": "2023-07-28T15:16:50.496792Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CTCLoss(labels, logits):\n",
    "    label_length = tf.reduce_sum(tf.cast(labels != pad_token_idx, tf.int32), axis=-1)\n",
    "    logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n",
    "    loss = tf.nn.ctc_loss(\n",
    "            labels=labels,\n",
    "            logits=logits,\n",
    "            label_length=label_length,\n",
    "            logit_length=logit_length,\n",
    "            blank_index=pad_token_idx,\n",
    "            logits_time_major=False\n",
    "        )\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2772a34f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T15:16:50.506408Z",
     "iopub.status.busy": "2023-07-28T15:16:50.505885Z",
     "iopub.status.idle": "2023-07-28T15:16:56.077558Z",
     "shell.execute_reply": "2023-07-28T15:16:56.076738Z",
     "shell.execute_reply.started": "2023-07-28T15:16:50.506374Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def get_model(dim=256):\n",
    "    inp = tf.keras.Input(INPUT_SHAPE)\n",
    "    x = tf.keras.layers.Masking(mask_value=0.0)(inp)\n",
    "    x = tf.keras.layers.Dense(dim, use_bias=False, name='stem_conv')(x)\n",
    "    pe = tf.cast(positional_encoding(INPUT_SHAPE[0], dim), dtype=x.dtype)\n",
    "    x = x + pe\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.95, name='stem_bn')(x)\n",
    "    \n",
    "    num_blocks = 8\n",
    "    drop_rate = 0.4\n",
    "    for i in range(num_blocks):\n",
    "        x = SqueezeformerBlock(\n",
    "            dim=dim,\n",
    "            num_heads=4,\n",
    "            expansion_factor=2,\n",
    "            kernel_size=15,\n",
    "            dropout=drop_rate\n",
    "        )(x)\n",
    "    \n",
    "    # Fixed these lines by adding (x) to call the layers\n",
    "    x = tf.keras.layers.Dense(dim, activation='relu', name='top_conv')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    x = tf.keras.layers.Dense(len(char_to_num), name='classifier')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inp, x)\n",
    "    \n",
    "    loss = CTCLoss\n",
    "    optimizer = tfa.optimizers.RectifiedAdam(sma_threshold=4)\n",
    "    optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = get_model()\n",
    "model(batch[0])\n",
    "model.summary()\n",
    "'''\n",
    "\n",
    "\n",
    "def get_model(input_shape=(None, len(SEL_COLS)), target_vocab_size=60, dim=256):\n",
    "    \"\"\"\n",
    "    Combines a Squeezeformer encoder and a Transformer-based decoder to predict ASL characters from landmark inputs.\n",
    "    \"\"\"\n",
    "    ## --- 1. Encoder Inputs (landmark features) ---\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape, name=\"encoder_inputs\")\n",
    "    \n",
    "    ## --- 2. Squeezeformer Encoder ---\n",
    "    x = tf.keras.layers.Masking(mask_value=0.0)(encoder_inputs)\n",
    "    x = tf.keras.layers.Dense(dim, use_bias=False, name='stem_conv')(x)\n",
    "    \n",
    "    # Use input_shape[1] instead of input_shape[0]\n",
    "    pe = tf.cast(positional_encoding(input_shape[1], dim), dtype=x.dtype)\n",
    "    x = x + pe\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.95, name='stem_bn')(x)\n",
    "    \n",
    "    # Squeezeformer blocks (as before)\n",
    "    num_blocks = 8\n",
    "    drop_rate = 0.4\n",
    "    for i in range(num_blocks):\n",
    "        x = SqueezeformerBlock(\n",
    "            dim=dim,\n",
    "            num_heads=4,\n",
    "            expansion_factor=2,\n",
    "            kernel_size=15,\n",
    "            dropout=drop_rate\n",
    "        )(x)\n",
    "    \n",
    "    encoder_outputs = x  # Encoder output to be used in the decoder\n",
    "    \n",
    "    ## --- 3. Decoder Inputs (character sequence for teacher forcing) ---\n",
    "    decoder_inputs = tf.keras.Input(shape=(None, dim), name=\"decoder_inputs\")\n",
    "    \n",
    "    ## --- 4. Squeezeformer-Transformer Decoder ---\n",
    "    x = decoder_inputs\n",
    "    num_decoder_blocks = 4\n",
    "    for _ in range(num_decoder_blocks):\n",
    "        x = SqueezeformerBlock(dim=dim, num_heads=4, expansion_factor=2, kernel_size=15, dropout=0.1)(x)\n",
    "    \n",
    "    # Cross-attention to encoder outputs\n",
    "    cross_attention = MultiHeadSelfAttention(dim=dim, num_heads=4)(x, encoder_outputs)\n",
    "    x = tf.keras.layers.LayerNormalization()(x + cross_attention)\n",
    "\n",
    "    # Final character classifier\n",
    "    decoder_outputs = tf.keras.layers.Dense(target_vocab_size, activation='softmax', name='decoder_output')(x)\n",
    "    \n",
    "    ## --- 5. Create Model ---\n",
    "    model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "    \n",
    "    ## --- 6. Compile Model ---\n",
    "    optimizer = tfa.optimizers.RectifiedAdam(sma_threshold=4)\n",
    "    optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n",
    "    model.compile(optimizer=optimizer, loss={'decoder_output': CTCLoss}, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = get_model(input_shape=(None, len(SEL_COLS)), target_vocab_size=len(char_to_num))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7628ce0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T15:16:56.079019Z",
     "iopub.status.busy": "2023-07-28T15:16:56.078661Z",
     "iopub.status.idle": "2023-07-28T15:16:56.095309Z",
     "shell.execute_reply": "2023-07-28T15:16:56.094333Z",
     "shell.execute_reply.started": "2023-07-28T15:16:56.078973Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_to_char_fn(y):\n",
    "    return [num_to_char.get(x, \"\") for x in y]\n",
    "\n",
    "@tf.function()\n",
    "def decode_phrase(pred):\n",
    "    x = tf.argmax(pred, axis=1)\n",
    "    diff = tf.not_equal(x[:-1], x[1:])\n",
    "    adjacent_indices = tf.where(diff)[:, 0]\n",
    "    x = tf.gather(x, adjacent_indices)\n",
    "    mask = x != pad_token_idx\n",
    "    x = tf.boolean_mask(x, mask, axis=0)\n",
    "    return x\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    output_text = []\n",
    "    for result in pred:\n",
    "        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n",
    "        output_text.append(result)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d9445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T15:16:56.097105Z",
     "iopub.status.busy": "2023-07-28T15:16:56.096747Z",
     "iopub.status.idle": "2023-07-28T15:16:56.12454Z",
     "shell.execute_reply": "2023-07-28T15:16:56.123391Z",
     "shell.execute_reply.started": "2023-07-28T15:16:56.097073Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# A callback class to output a few transcriptions during training\n",
    "class CallbackEval(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs=None):\n",
    "        model.save_weights(\"model.h5\")\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for batch in self.dataset:\n",
    "            X, y = batch\n",
    "            batch_predictions = model(X)\n",
    "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
    "            predictions.extend(batch_predictions)\n",
    "            for label in y:\n",
    "                label = \"\".join(num_to_char_fn(label.numpy()))\n",
    "                targets.append(label)\n",
    "        print(\"-\" * 100)\n",
    "        # for i in np.random.randint(0, len(predictions), 2):\n",
    "        for i in range(32):\n",
    "            print(f\"Target    : {targets[i]}\")\n",
    "            print(f\"Prediction: {predictions[i]}, len: {len(predictions[i])}\")\n",
    "            print(\"-\" * 100)\n",
    "\n",
    "# Callback function to check transcription on the val set.\n",
    "validation_callback = CallbackEval(val_dataset.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b15330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T15:16:56.126629Z",
     "iopub.status.busy": "2023-07-28T15:16:56.126121Z",
     "iopub.status.idle": "2023-07-28T15:16:56.137595Z",
     "shell.execute_reply": "2023-07-28T15:16:56.136422Z",
     "shell.execute_reply.started": "2023-07-28T15:16:56.126561Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 2 if IS_INTERACTIVE else 50\n",
    "N_WARMUP_EPOCHS = 0 if IS_INTERACTIVE else 5\n",
    "LR_MAX = 4e-3\n",
    "WD_RATIO = 0.05\n",
    "WARMUP_METHOD = \"exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1d01a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T15:16:56.140391Z",
     "iopub.status.busy": "2023-07-28T15:16:56.139501Z",
     "iopub.status.idle": "2023-07-28T15:16:56.794762Z",
     "shell.execute_reply": "2023-07-28T15:16:56.793817Z",
     "shell.execute_reply.started": "2023-07-28T15:16:56.1403Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n",
    "    \n",
    "    if current_step < num_warmup_steps:\n",
    "        if WARMUP_METHOD == 'log':\n",
    "            return lr_max * 0.10 ** (num_warmup_steps - current_step)\n",
    "        else:\n",
    "            return lr_max * 2 ** -(num_warmup_steps - current_step)\n",
    "    else:\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max\n",
    "    \n",
    "def plot_lr_schedule(lr_schedule, epochs):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    plt.plot([None] + lr_schedule + [None])\n",
    "    # X Labels\n",
    "    x = np.arange(1, epochs + 1)\n",
    "    x_axis_labels = [i if epochs <= 40 or i % 5 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n",
    "    plt.xlim([1, epochs])\n",
    "    plt.xticks(x, x_axis_labels) # set tick step to 1 and let x axis start at 1\n",
    "    \n",
    "    # Increase y-limit for better readability\n",
    "    plt.ylim([0, max(lr_schedule) * 1.1])\n",
    "    \n",
    "    # Title\n",
    "    schedule_info = f'start: {lr_schedule[0]:.1E}, max: {max(lr_schedule):.1E}, final: {lr_schedule[-1]:.1E}'\n",
    "    plt.title(f'Step Learning Rate Schedule, {schedule_info}', size=18, pad=12)\n",
    "    \n",
    "    # Plot Learning Rates\n",
    "    for x, val in enumerate(lr_schedule):\n",
    "        if epochs <= 40 or x % 5 == 0 or x is epochs - 1:\n",
    "            if x < len(lr_schedule) - 1:\n",
    "                if lr_schedule[x - 1] < val:\n",
    "                    ha = 'right'\n",
    "                else:\n",
    "                    ha = 'left'\n",
    "            elif x == 0:\n",
    "                ha = 'right'\n",
    "            else:\n",
    "                ha = 'left'\n",
    "            plt.plot(x + 1, val, 'o', color='black');\n",
    "            offset_y = (max(lr_schedule) - min(lr_schedule)) * 0.02\n",
    "            plt.annotate(f'{val:.1E}', xy=(x + 1, val + offset_y), size=12, ha=ha)\n",
    "    \n",
    "    plt.xlabel('Epoch', size=16, labelpad=5)\n",
    "    plt.ylabel('Learning Rate', size=16, labelpad=5)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Learning rate for encoder\n",
    "LR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\n",
    "# Plot Learning Rate Schedule\n",
    "plot_lr_schedule(LR_SCHEDULE, epochs=N_EPOCHS)\n",
    "# Learning Rate Callback\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)\n",
    "\n",
    "# Custom callback to update weight decay with learning rate\n",
    "class WeightDecayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, wd_ratio=WD_RATIO):\n",
    "        self.step_counter = 0\n",
    "        self.wd_ratio = wd_ratio\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n",
    "        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bf623",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T15:16:56.799851Z",
     "iopub.status.busy": "2023-07-28T15:16:56.799121Z",
     "iopub.status.idle": "2023-07-28T15:17:30.607822Z",
     "shell.execute_reply": "2023-07-28T15:17:30.604703Z",
     "shell.execute_reply.started": "2023-07-28T15:16:56.799813Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=N_EPOCHS,\n",
    "    callbacks=[\n",
    "        validation_callback,\n",
    "        lr_callback,\n",
    "        WeightDecayCallback(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a590c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-28T15:17:30.611295Z",
     "iopub.status.idle": "2023-07-28T15:17:30.613808Z",
     "shell.execute_reply": "2023-07-28T15:17:30.613558Z",
     "shell.execute_reply.started": "2023-07-28T15:17:30.613532Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TFLiteModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs, training=False):\n",
    "        # Preprocess Data\n",
    "        x = tf.cast(inputs, tf.float32)\n",
    "        x = x[None]\n",
    "        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(SEL_COLS))), lambda: tf.identity(x))\n",
    "        x = x[0]\n",
    "        x = pre_process00(x)\n",
    "        x = pre_process1(*x)\n",
    "        x = tf.reshape(x, INPUT_SHAPE)\n",
    "        x = x[None]\n",
    "        x = self.model(x, training=False)\n",
    "        x = x[0]\n",
    "        x = decode_phrase(x)\n",
    "        # POST PROCESS. IF PRED LESS THAN 3, USE CONSTANT PREDICTION FROM\n",
    "        # https://www.kaggle.com/code/anokas/static-greedy-baseline-0-157-lb\n",
    "        x = tf.cond(tf.shape(x)[0] < 3, lambda: tf.constant(\n",
    "            [17, 0, 32, 12, 36, 0, 12, 32, 49, 46, 36], tf.int64), lambda: tf.identity(x))\n",
    "        x = tf.one_hot(x, 59)\n",
    "        return {'outputs': x}\n",
    "\n",
    "tflitemodel_base = TFLiteModel(model)\n",
    "tflitemodel_base(frames)[\"outputs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5071c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-28T15:17:30.617936Z",
     "iopub.status.idle": "2023-07-28T15:17:30.620425Z",
     "shell.execute_reply": "2023-07-28T15:17:30.62016Z",
     "shell.execute_reply.started": "2023-07-28T15:17:30.620132Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n",
    "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "keras_model_converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = keras_model_converter.convert()\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "with open('inference_args.json', \"w\") as f:\n",
    "    json.dump({\"selected_columns\" : SEL_COLS}, f)\n",
    "    \n",
    "!zip submission.zip  './model.tflite' './inference_args.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e4a02",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-28T15:17:30.622044Z",
     "iopub.status.idle": "2023-07-28T15:17:30.626835Z",
     "shell.execute_reply": "2023-07-28T15:17:30.626588Z",
     "shell.execute_reply.started": "2023-07-28T15:17:30.626563Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open (\"inference_args.json\", \"r\") as f:\n",
    "    SEL_COLS = json.load(f)[\"selected_columns\"]\n",
    "    \n",
    "def load_relevant_data_subset(pq_path):\n",
    "    return pd.read_parquet(pq_path, columns=SEL_COLS)\n",
    "\n",
    "def create_data_gen(file_ids, y_mul=1):\n",
    "    def gen():\n",
    "        for file_id in file_ids:\n",
    "            pqfile = f\"{inpdir}/{file_id}.parquet\"\n",
    "            seq_refs = df.loc[df.file_id == file_id]\n",
    "            seqs = load_relevant_data_subset(pqfile)\n",
    "\n",
    "            for seq_id in seq_refs.sequence_id:\n",
    "                x = seqs.iloc[seqs.index == seq_id].to_numpy()\n",
    "                y = str(df.loc[df.sequence_id == seq_id].phrase.iloc[0])\n",
    "                \n",
    "                r_nonan = np.sum(np.sum(np.isnan(x[:, RHAND_IDX_X]), axis = 1) == 0)\n",
    "                l_nonan = np.sum(np.sum(np.isnan(x[:, LHAND_IDX_X]), axis = 1) == 0)\n",
    "                no_nan = max(r_nonan, l_nonan)\n",
    "                \n",
    "                if y_mul*len(y)<no_nan:\n",
    "                    yield x, y\n",
    "    return gen\n",
    "\n",
    "pqfiles = df.file_id.unique()\n",
    "val_len = int(0.05 * len(pqfiles))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(create_data_gen(pqfiles[:val_len], 0),\n",
    "    output_signature=(tf.TensorSpec(shape=(None, len(SEL_COLS)), dtype=tf.float32), tf.TensorSpec(shape=(), dtype=tf.string))\n",
    ").prefetch(buffer_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8be100",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-28T15:17:30.628442Z",
     "iopub.status.idle": "2023-07-28T15:17:30.629262Z",
     "shell.execute_reply": "2023-07-28T15:17:30.629036Z",
     "shell.execute_reply.started": "2023-07-28T15:17:30.629011Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(\"model.tflite\")\n",
    "\n",
    "REQUIRED_SIGNATURE = \"serving_default\"\n",
    "REQUIRED_OUTPUT = \"outputs\"\n",
    "\n",
    "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    character_map = json.load(f)\n",
    "rev_character_map = {j:i for i,j in character_map.items()}\n",
    "\n",
    "prediction_fn = interpreter.get_signature_runner(REQUIRED_SIGNATURE)\n",
    "\n",
    "for frame, target in test_dataset.skip(100).take(10):\n",
    "    output = prediction_fn(inputs=frame)\n",
    "    prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n",
    "    target = target.numpy().decode(\"utf-8\")\n",
    "    print(\"pred =\", prediction_str, \"; target =\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee6082",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-28T15:17:30.63078Z",
     "iopub.status.idle": "2023-07-28T15:17:30.631608Z",
     "shell.execute_reply": "2023-07-28T15:17:30.631345Z",
     "shell.execute_reply.started": "2023-07-28T15:17:30.631322Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "output = prediction_fn(inputs=frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd52b1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-28T15:17:30.639192Z",
     "iopub.status.idle": "2023-07-28T15:17:30.639959Z",
     "shell.execute_reply": "2023-07-28T15:17:30.639749Z",
     "shell.execute_reply.started": "2023-07-28T15:17:30.639727Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i, (frame, target) in tqdm(enumerate(test_dataset.take(1000))):\n",
    "    output = prediction_fn(inputs=frame)\n",
    "    prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n",
    "    target = target.numpy().decode(\"utf-8\")\n",
    "    score = (len(target) - distance(prediction_str, target)) / len(target)\n",
    "    scores.append(score)\n",
    "    if i % 50 == 0:\n",
    "        print(np.sum(scores) / len(scores))\n",
    "    \n",
    "scores = np.array(scores)\n",
    "print(np.sum(scores) / len(scores))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 5973250,
     "sourceId": 52950,
     "sourceType": "competition"
    },
    {
     "datasetId": 3497052,
     "sourceId": 6182721,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3651580,
     "sourceId": 6342181,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30512,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.545337,
   "end_time": "2024-12-08T20:05:57.621647",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-08T20:05:31.076310",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
