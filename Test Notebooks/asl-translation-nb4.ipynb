{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T08:53:03.525496Z",
     "iopub.status.busy": "2024-11-29T08:53:03.524994Z",
     "iopub.status.idle": "2024-11-29T08:53:13.921986Z",
     "shell.execute_reply": "2024-11-29T08:53:13.920998Z",
     "shell.execute_reply.started": "2024-11-29T08:53:03.525440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install python-levenshtein tqdm pyarrow wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-29T08:53:13.924471Z",
     "iopub.status.busy": "2024-11-29T08:53:13.924161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import tensorflow_addons as tfa\n",
    "import Levenshtein as lev\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "wandb.login(key=\"afe8b8c0a3f1c1339a3daa9f619cb7c311218022\")\n",
    "wandb.init(project=\"asl-translation\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seeds()\n",
    "\n",
    "# Landmark indices and configurations\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE\n",
    "FRAME_LEN = 128  # Reduced from 384 to match competition\n",
    "CHANNELS = 384  # Model dimension\n",
    "\n",
    "# Create feature columns for landmarks\n",
    "X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\n",
    "Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\n",
    "Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n",
    "SEL_COLS = X + Y + Z\n",
    "\n",
    "# Hand and pose indices\n",
    "RHAND_IDX = [i for i, x in enumerate(X) if 'right_hand' in x]\n",
    "LHAND_IDX = [i for i, x in enumerate(X) if 'left_hand' in x]\n",
    "POSE_IDX = [i for i, x in enumerate(X) if 'pose' in x]\n",
    "\n",
    "# Special tokens\n",
    "pad_token = 'P'\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "pad_token_idx = 59\n",
    "start_token_idx = 60\n",
    "end_token_idx = 61\n",
    "\n",
    "# Training configuration\n",
    "BATCH_SIZE = 64 * 4  # Increased batch size\n",
    "MAX_LEN = 340  # Maximum sequence length\n",
    "DEBUG = False  # Debug mode flag\n",
    "N_EPOCHS = 1500\n",
    "N_WARMUP_EPOCHS = 5\n",
    "LR_MAX = 1e-3\n",
    "WD_RATIO = 0.05\n",
    "\n",
    "# Configure TPU if available\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    print(\"Using TPU:\", tpu.master())\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Using default strategy:\", strategy)\n",
    "\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "# Adjust batch size based on strategy\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "# Load character mapping\n",
    "def load_character_map(vocab_path):\n",
    "    with open(vocab_path, \"r\") as f:\n",
    "        char_to_num = json.load(f)\n",
    "    \n",
    "    # Add special tokens\n",
    "    char_to_num[pad_token] = pad_token_idx\n",
    "    char_to_num[start_token] = start_token_idx\n",
    "    char_to_num[end_token] = end_token_idx\n",
    "    \n",
    "    num_to_char = {j:i for i,j in char_to_num.items()}\n",
    "    return char_to_num, num_to_char\n",
    "\n",
    "# Data augmentation functions\n",
    "def spatial_random_affine(xyz,\n",
    "    scale=(0.8, 1.2),\n",
    "    shear=(-0.15, 0.15),\n",
    "    shift=(-0.1, 0.1),\n",
    "    degree=(-30, 30)):\n",
    "    \n",
    "    xy = xyz[..., :2]\n",
    "    z = xyz[..., 2:]\n",
    "    \n",
    "    # Apply scale\n",
    "    if scale is not None:\n",
    "        scale = tf.random.uniform((), *scale)\n",
    "        xy *= scale\n",
    "    \n",
    "    # Apply shear\n",
    "    if shear is not None:\n",
    "        shear_x = shear_y = tf.random.uniform((), *shear)\n",
    "        if tf.random.uniform(()) < 0.5:\n",
    "            shear_x = 0.\n",
    "        else:\n",
    "            shear_y = 0.\n",
    "        shear_mat = tf.constant([[1., shear_x], [shear_y, 1.]], dtype=xy.dtype)\n",
    "        xy = tf.matmul(xy, shear_mat)\n",
    "    \n",
    "    # Apply rotation\n",
    "    if degree is not None:\n",
    "        angle = tf.random.uniform((), *degree) * math.pi / 180\n",
    "        cos = tf.cos(angle)\n",
    "        sin = tf.sin(angle)\n",
    "        rot_mat = tf.constant([[cos, -sin], [sin, cos]], dtype=xy.dtype)\n",
    "        xy = tf.matmul(xy, rot_mat)\n",
    "    \n",
    "    # Apply shift\n",
    "    if shift is not None:\n",
    "        shift = tf.random.uniform((), *shift)\n",
    "        xy += shift\n",
    "    \n",
    "    return tf.concat([xy, z], axis=-1)\n",
    "\n",
    "def temporal_mask(x, size=(0.2, 0.4), mask_value=float('nan')):\n",
    "    length = tf.shape(x)[0]\n",
    "    mask_size = tf.random.uniform((), *size)\n",
    "    mask_size = tf.cast(tf.cast(length, tf.float32) * mask_size, tf.int32)\n",
    "    mask_offset = tf.random.uniform((), 0, length - mask_size, dtype=tf.int32)\n",
    "    \n",
    "    mask = tf.ones((length,), dtype=tf.bool)\n",
    "    mask_indices = tf.range(mask_offset, mask_offset + mask_size)\n",
    "    mask = tf.tensor_scatter_nd_update(mask, mask_indices[:, tf.newaxis], tf.zeros_like(mask_indices, dtype=tf.bool))\n",
    "    \n",
    "    masked_x = tf.where(mask[:, tf.newaxis, tf.newaxis], x, mask_value)\n",
    "    return masked_x\n",
    "\n",
    "def augment_fn(x, always=False):\n",
    "    if tf.random.uniform(()) < 0.75 or always:\n",
    "        x = spatial_random_affine(x)\n",
    "    if tf.random.uniform(()) < 0.5 or always:\n",
    "        x = temporal_mask(x)\n",
    "    return x\n",
    "\n",
    "# Model architecture components\n",
    "class SqueezeformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Self attention\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mhsa = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=dim//num_heads,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        # First feedforward\n",
    "        self.ff1_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ff1 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim*4, activation='swish'),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(dim),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        \n",
    "        # Convolution module\n",
    "        self.conv_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.conv1 = tf.keras.layers.Conv1D(dim*2, 1)\n",
    "        self.glu = tf.keras.layers.Lambda(lambda x: x[:,:,:dim] * tf.nn.sigmoid(x[:,:,dim:]))\n",
    "        self.depthwise_conv = tf.keras.layers.DepthwiseConv1D(3, padding='same')\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        self.pointwise_conv = tf.keras.layers.Conv1D(dim, 1)\n",
    "        self.conv_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # Second feedforward\n",
    "        self.ff2_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ff2 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim*4, activation='swish'),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(dim),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        # First feedforward\n",
    "        residual = x\n",
    "        x = self.ff1_norm(x)\n",
    "        x = self.ff1(x, training=training)\n",
    "        x = residual + x\n",
    "        \n",
    "        # Self attention\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.mhsa(x, x, training=training)\n",
    "        x = residual + x\n",
    "        \n",
    "        # Convolution module\n",
    "        residual = x\n",
    "        x = self.conv_norm(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.glu(x)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.batch_norm(x, training=training)\n",
    "        x = self.pointwise_conv(x)\n",
    "        x = self.conv_dropout(x, training=training)\n",
    "        x = residual + x\n",
    "        \n",
    "        # Second feedforward\n",
    "        residual = x\n",
    "        x = self.ff2_norm(x)\n",
    "        x = self.ff2(x, training=training)\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ASLModel(tf.keras.Model):\n",
    "    def __init__(self, num_classes=62, dim=384, num_layers=4, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Input processing\n",
    "        self.input_projection = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "        self.input_norm = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Feature extractors\n",
    "        self.face_projection = tf.keras.layers.Dense(dim//4)\n",
    "        self.pose_projection = tf.keras.layers.Dense(dim//4)\n",
    "        self.hand_projection = tf.keras.layers.Dense(dim//2)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = [\n",
    "            SqueezeformerBlock(dim, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # Output layers\n",
    "        self.pre_classifier = tf.keras.layers.Dense(dim*2, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        # Project and normalize input\n",
    "        x = self.input_projection(inputs)\n",
    "        x = self.input_norm(x, training=training)\n",
    "        \n",
    "        # Process through encoder layers\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, training=training)\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.pre_classifier(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Data processing functions\n",
    "def process_landmarks(data):\n",
    "    \"\"\"Extract and normalize landmarks\"\"\"\n",
    "    # Extract hand landmarks\n",
    "    right_hand = tf.gather(data, RHAND_IDX, axis=1)\n",
    "    left_hand = tf.gather(data, LHAND_IDX, axis=1)\n",
    "    pose = tf.gather(data, POSE_IDX, axis=1)\n",
    "    \n",
    "    # Detect dominant hand\n",
    "    right_nan = tf.reduce_sum(tf.cast(tf.math.is_nan(right_hand), tf.float32))\n",
    "    left_nan = tf.reduce_sum(tf.cast(tf.math.is_nan(left_hand), tf.float32))\n",
    "    \n",
    "    # Use hand with fewer NaN values\n",
    "    hand = tf.cond(right_nan > left_nan,\n",
    "                  lambda: left_hand,\n",
    "                  lambda: right_hand)\n",
    "    \n",
    "    # Normalize coordinates\n",
    "    mean = tf.reduce_mean(hand, axis=1, keepdims=True)\n",
    "    std = tf.math.reduce_std(hand, axis=1, keepdims=True)\n",
    "    std = tf.where(std < 1e-6, 1.0, std)\n",
    "    \n",
    "    hand = (hand - mean) / std\n",
    "    pose = (pose - mean) / std\n",
    "    \n",
    "    # Combine features\n",
    "    features = tf.concat([hand, pose], axis=1)\n",
    "    return features\n",
    "\n",
    "def create_tf_dataset(file_paths, char_to_num, batch_size=BATCH_SIZE, is_training=False):\n",
    "    \"\"\"Create TensorFlow dataset from TFRecord files\"\"\"\n",
    "    \n",
    "    def parse_tfrecord(example_proto):\n",
    "        feature_description = {\n",
    "            'frames': tf.io.VarLenFeature(tf.float32),\n",
    "            'phrase': tf.io.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "        # Process frames\n",
    "        frames = tf.sparse.to_dense(parsed_features['frames'])\n",
    "        frames = tf.reshape(frames, [-1, len(SEL_COLS)])\n",
    "        frames = process_landmarks(frames)\n",
    "        \n",
    "        if is_training:\n",
    "            frames = augment_fn(frames)\n",
    "        \n",
    "        # Process phrase\n",
    "        phrase = parsed_features['phrase']\n",
    "        phrase = tf.strings.unicode_split(phrase, 'UTF-8')\n",
    "        phrase = table.lookup(phrase)\n",
    "        phrase = tf.pad(phrase, [[0, MAX_LEN - tf.shape(phrase)[0]]], constant_values=pad_token_idx)\n",
    "        \n",
    "        return frames, phrase\n",
    "    \n",
    "    # Create lookup table\n",
    "    table = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(\n",
    "            keys=list(char_to_num.keys()),\n",
    "            values=list(char_to_num.values()),\n",
    "        ),\n",
    "        default_value=pad_token_idx\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.TFRecordDataset(file_paths, compression_type='GZIP')\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(10000)\n",
    "    \n",
    "    dataset = (dataset\n",
    "              .map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "              .batch(batch_size)\n",
    "              .prefetch(tf.data.AUTOTUNE))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Loss and metrics functions\n",
    "def CTCLoss(labels, logits):\n",
    "    \"\"\"CTC Loss implementation\"\"\"\n",
    "    label_length = tf.reduce_sum(tf.cast(labels != pad_token_idx, tf.int32), axis=-1)\n",
    "    logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n",
    "    \n",
    "    loss = tf.nn.ctc_loss(\n",
    "        labels=tf.cast(labels, tf.int32),\n",
    "        logits=logits,\n",
    "        label_length=label_length,\n",
    "        logit_length=logit_length,\n",
    "        blank_index=pad_token_idx,\n",
    "        logits_time_major=False\n",
    "    )\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def calculate_levenshtein(predictions, targets, num_to_char):\n",
    "    \"\"\"Calculate normalized Levenshtein distance\"\"\"\n",
    "    distances = []\n",
    "    prediction_lengths = []\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        pred_text = ''.join([num_to_char.get(x, '') for x in pred if x not in [pad_token_idx, start_token_idx, end_token_idx]])\n",
    "        target_text = ''.join([num_to_char.get(x, '') for x in target if x not in [pad_token_idx, start_token_idx, end_token_idx]])\n",
    "        \n",
    "        distance = lev.distance(pred_text, target_text)\n",
    "        distances.append(distance)\n",
    "        prediction_lengths.append(len(target_text))\n",
    "    \n",
    "    return 1 - (sum(distances) / sum(prediction_lengths))\n",
    "\n",
    "# Learning rate schedule\n",
    "def get_lr_schedule(initial_lr, warmup_epochs, total_epochs, steps_per_epoch):\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    total_steps = total_epochs * steps_per_epoch\n",
    "    \n",
    "    def lr_schedule(step):\n",
    "        # Linear warmup\n",
    "        if step < warmup_steps:\n",
    "            return initial_lr * (step / warmup_steps)\n",
    "        \n",
    "        # Cosine decay\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        return initial_lr * 0.5 * (1 + tf.cos(math.pi * progress))\n",
    "    \n",
    "    return lr_schedule\n",
    "\n",
    "# Custom callbacks\n",
    "class LevenshteinCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, num_to_char):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.num_to_char = num_to_char\n",
    "        self.best_score = -float('inf')\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 5 == 0:  # Calculate every 5 epochs to save time\n",
    "            predictions = []\n",
    "            targets = []\n",
    "            \n",
    "            for batch in self.validation_data:\n",
    "                pred = self.model(batch[0], training=False)\n",
    "                pred = tf.argmax(pred, axis=-1)\n",
    "                predictions.extend(pred.numpy())\n",
    "                targets.extend(batch[1].numpy())\n",
    "            \n",
    "            score = calculate_levenshtein(predictions, targets, self.num_to_char)\n",
    "            logs['levenshtein_score'] = score\n",
    "            \n",
    "            if score > self.best_score:\n",
    "                self.best_score = score\n",
    "                self.model.save_weights(f'best_model_epoch_{epoch}.h5')\n",
    "            \n",
    "            print(f'\\nLevenshtein Score: {score:.4f}')\n",
    "\n",
    "# Training function\n",
    "def train_model(strategy, train_dataset, val_dataset, num_to_char, config):\n",
    "    with strategy.scope():\n",
    "        # Create model\n",
    "        model = ASLModel(\n",
    "            num_classes=len(num_to_char),\n",
    "            dim=config['dim'],\n",
    "            num_layers=config['num_layers'],\n",
    "            dropout_rate=config['dropout_rate']\n",
    "        )\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = tfa.optimizers.RectifiedAdam(\n",
    "            learning_rate=config['initial_lr'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "            sma_threshold=4.0\n",
    "        )\n",
    "        optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=CTCLoss,\n",
    "            #run_eagerly=True  # For debugging\n",
    "        )\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    steps_per_epoch = len(train_dataset)\n",
    "    lr_schedule = get_lr_schedule(\n",
    "        config['initial_lr'],\n",
    "        config['warmup_epochs'],\n",
    "        config['epochs'],\n",
    "        steps_per_epoch\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.LearningRateScheduler(lr_schedule),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'checkpoint_epoch_{epoch}.h5',\n",
    "            save_weights_only=True,\n",
    "            save_freq='epoch'\n",
    "        ),\n",
    "        LevenshteinCallback(val_dataset, num_to_char),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=config['epochs'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'dim': 384,\n",
    "        'num_layers': 4,\n",
    "        'dropout_rate': 0.1,\n",
    "        'initial_lr': 1e-3,\n",
    "        'weight_decay': 0.05,\n",
    "        'warmup_epochs': 5,\n",
    "        'epochs': 100,\n",
    "        'batch_size': GLOBAL_BATCH_SIZE\n",
    "    }\n",
    "    \n",
    "    # Load character mapping\n",
    "    char_to_num, num_to_char = load_character_map('character_to_prediction_index.json')\n",
    "    \n",
    "    # Create TFRecord datasets\n",
    "    train_files = sorted(tf.io.gfile.glob('train/*.tfrecord'))\n",
    "    val_files = sorted(tf.io.gfile.glob('val/*.tfrecord'))\n",
    "    \n",
    "    train_dataset = create_tf_dataset(\n",
    "        train_files,\n",
    "        char_to_num,\n",
    "        batch_size=config['batch_size'],\n",
    "        is_training=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = create_tf_dataset(\n",
    "        val_files,\n",
    "        char_to_num,\n",
    "        batch_size=config['batch_size'],\n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_model(\n",
    "        strategy,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        num_to_char,\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['levenshtein_score'])\n",
    "    plt.title('Levenshtein Score')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save final model\n",
    "    model.save_weights('final_model.h5')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 5973250,
     "sourceId": 52950,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
