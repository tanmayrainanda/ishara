{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":52950,"databundleVersionId":5973250,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install python-levenshtein tqdm pyarrow wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:53:03.524994Z","iopub.execute_input":"2024-11-29T08:53:03.525496Z","iopub.status.idle":"2024-11-29T08:53:13.921986Z","shell.execute_reply.started":"2024-11-29T08:53:03.525440Z","shell.execute_reply":"2024-11-29T08:53:13.920998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport numpy as np\nimport random\nimport json\nimport math\nimport pickle\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport tensorflow_addons as tfa\nimport Levenshtein as lev\nimport pyarrow.parquet as pq\nfrom tqdm.auto import tqdm\n\n# Set random seeds for reproducibility\nSEED = 42\ndef set_seeds(seed=SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\nset_seeds()\n\n# Landmark indices and configurations\nLPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE\nFRAME_LEN = 128  # Reduced from 384 to match competition\nCHANNELS = 384  # Model dimension\n\n# Create feature columns for landmarks\nX = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\nSEL_COLS = X + Y + Z\n\n# Hand and pose indices\nRHAND_IDX = [i for i, x in enumerate(X) if 'right_hand' in x]\nLHAND_IDX = [i for i, x in enumerate(X) if 'left_hand' in x]\nPOSE_IDX = [i for i, x in enumerate(X) if 'pose' in x]\n\n# Special tokens\npad_token = 'P'\nstart_token = '<'\nend_token = '>'\npad_token_idx = 59\nstart_token_idx = 60\nend_token_idx = 61\n\n# Training configuration\nBATCH_SIZE = 64 * 4  # Increased batch size\nMAX_LEN = 340  # Maximum sequence length\nDEBUG = False  # Debug mode flag\nN_EPOCHS = 1500\nN_WARMUP_EPOCHS = 5\nLR_MAX = 1e-3\nWD_RATIO = 0.05\n\n# Configure TPU if available\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print(\"Using TPU:\", tpu.master())\nexcept:\n    strategy = tf.distribute.get_strategy()\n    print(\"Using default strategy:\", strategy)\n\nprint(\"Number of replicas:\", strategy.num_replicas_in_sync)\n\n# Adjust batch size based on strategy\nGLOBAL_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\n\n# Load character mapping\ndef load_character_map(vocab_path):\n    with open(vocab_path, \"r\") as f:\n        char_to_num = json.load(f)\n    \n    # Add special tokens\n    char_to_num[pad_token] = pad_token_idx\n    char_to_num[start_token] = start_token_idx\n    char_to_num[end_token] = end_token_idx\n    \n    num_to_char = {j:i for i,j in char_to_num.items()}\n    return char_to_num, num_to_char\n\n# Data augmentation functions\ndef spatial_random_affine(xyz,\n    scale=(0.8, 1.2),\n    shear=(-0.15, 0.15),\n    shift=(-0.1, 0.1),\n    degree=(-30, 30)):\n    \n    xy = xyz[..., :2]\n    z = xyz[..., 2:]\n    \n    # Apply scale\n    if scale is not None:\n        scale = tf.random.uniform((), *scale)\n        xy *= scale\n    \n    # Apply shear\n    if shear is not None:\n        shear_x = shear_y = tf.random.uniform((), *shear)\n        if tf.random.uniform(()) < 0.5:\n            shear_x = 0.\n        else:\n            shear_y = 0.\n        shear_mat = tf.constant([[1., shear_x], [shear_y, 1.]], dtype=xy.dtype)\n        xy = tf.matmul(xy, shear_mat)\n    \n    # Apply rotation\n    if degree is not None:\n        angle = tf.random.uniform((), *degree) * math.pi / 180\n        cos = tf.cos(angle)\n        sin = tf.sin(angle)\n        rot_mat = tf.constant([[cos, -sin], [sin, cos]], dtype=xy.dtype)\n        xy = tf.matmul(xy, rot_mat)\n    \n    # Apply shift\n    if shift is not None:\n        shift = tf.random.uniform((), *shift)\n        xy += shift\n    \n    return tf.concat([xy, z], axis=-1)\n\ndef temporal_mask(x, size=(0.2, 0.4), mask_value=float('nan')):\n    length = tf.shape(x)[0]\n    mask_size = tf.random.uniform((), *size)\n    mask_size = tf.cast(tf.cast(length, tf.float32) * mask_size, tf.int32)\n    mask_offset = tf.random.uniform((), 0, length - mask_size, dtype=tf.int32)\n    \n    mask = tf.ones((length,), dtype=tf.bool)\n    mask_indices = tf.range(mask_offset, mask_offset + mask_size)\n    mask = tf.tensor_scatter_nd_update(mask, mask_indices[:, tf.newaxis], tf.zeros_like(mask_indices, dtype=tf.bool))\n    \n    masked_x = tf.where(mask[:, tf.newaxis, tf.newaxis], x, mask_value)\n    return masked_x\n\ndef augment_fn(x, always=False):\n    if tf.random.uniform(()) < 0.75 or always:\n        x = spatial_random_affine(x)\n    if tf.random.uniform(()) < 0.5 or always:\n        x = temporal_mask(x)\n    return x\n\n# Model architecture components\nclass SqueezeformerBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, num_heads=8, dropout_rate=0.1):\n        super().__init__()\n        self.dim = dim\n        \n        # Self attention\n        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.mhsa = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads, \n            key_dim=dim//num_heads,\n            dropout=dropout_rate\n        )\n        \n        # First feedforward\n        self.ff1_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.ff1 = tf.keras.Sequential([\n            tf.keras.layers.Dense(dim*4, activation='swish'),\n            tf.keras.layers.Dropout(dropout_rate),\n            tf.keras.layers.Dense(dim),\n            tf.keras.layers.Dropout(dropout_rate)\n        ])\n        \n        # Convolution module\n        self.conv_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.conv1 = tf.keras.layers.Conv1D(dim*2, 1)\n        self.glu = tf.keras.layers.Lambda(lambda x: x[:,:,:dim] * tf.nn.sigmoid(x[:,:,dim:]))\n        self.depthwise_conv = tf.keras.layers.DepthwiseConv1D(3, padding='same')\n        self.batch_norm = tf.keras.layers.BatchNormalization()\n        self.pointwise_conv = tf.keras.layers.Conv1D(dim, 1)\n        self.conv_dropout = tf.keras.layers.Dropout(dropout_rate)\n        \n        # Second feedforward\n        self.ff2_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.ff2 = tf.keras.Sequential([\n            tf.keras.layers.Dense(dim*4, activation='swish'),\n            tf.keras.layers.Dropout(dropout_rate),\n            tf.keras.layers.Dense(dim),\n            tf.keras.layers.Dropout(dropout_rate)\n        ])\n        \n    def call(self, x, training=False):\n        # First feedforward\n        residual = x\n        x = self.ff1_norm(x)\n        x = self.ff1(x, training=training)\n        x = residual + x\n        \n        # Self attention\n        residual = x\n        x = self.norm1(x)\n        x = self.mhsa(x, x, training=training)\n        x = residual + x\n        \n        # Convolution module\n        residual = x\n        x = self.conv_norm(x)\n        x = self.conv1(x)\n        x = self.glu(x)\n        x = self.depthwise_conv(x)\n        x = self.batch_norm(x, training=training)\n        x = self.pointwise_conv(x)\n        x = self.conv_dropout(x, training=training)\n        x = residual + x\n        \n        # Second feedforward\n        residual = x\n        x = self.ff2_norm(x)\n        x = self.ff2(x, training=training)\n        x = residual + x\n        \n        return x\n\nclass ASLModel(tf.keras.Model):\n    def __init__(self, num_classes=62, dim=384, num_layers=4, dropout_rate=0.1):\n        super().__init__()\n        self.dim = dim\n        \n        # Input processing\n        self.input_projection = tf.keras.layers.Dense(dim, use_bias=False)\n        self.input_norm = tf.keras.layers.BatchNormalization()\n        \n        # Feature extractors\n        self.face_projection = tf.keras.layers.Dense(dim//4)\n        self.pose_projection = tf.keras.layers.Dense(dim//4)\n        self.hand_projection = tf.keras.layers.Dense(dim//2)\n        \n        # Encoder layers\n        self.encoder_layers = [\n            SqueezeformerBlock(dim, dropout_rate=dropout_rate)\n            for _ in range(num_layers)\n        ]\n        \n        # Output layers\n        self.pre_classifier = tf.keras.layers.Dense(dim*2, activation='relu')\n        self.dropout = tf.keras.layers.Dropout(0.5)\n        self.classifier = tf.keras.layers.Dense(num_classes)\n    \n    def call(self, inputs, training=False):\n        # Project and normalize input\n        x = self.input_projection(inputs)\n        x = self.input_norm(x, training=training)\n        \n        # Process through encoder layers\n        for encoder_layer in self.encoder_layers:\n            x = encoder_layer(x, training=training)\n        \n        # Final classification\n        x = self.pre_classifier(x)\n        x = self.dropout(x, training=training)\n        x = self.classifier(x)\n        \n        return x\n\n# Data processing functions\ndef process_landmarks(data):\n    \"\"\"Extract and normalize landmarks\"\"\"\n    # Extract hand landmarks\n    right_hand = tf.gather(data, RHAND_IDX, axis=1)\n    left_hand = tf.gather(data, LHAND_IDX, axis=1)\n    pose = tf.gather(data, POSE_IDX, axis=1)\n    \n    # Detect dominant hand\n    right_nan = tf.reduce_sum(tf.cast(tf.math.is_nan(right_hand), tf.float32))\n    left_nan = tf.reduce_sum(tf.cast(tf.math.is_nan(left_hand), tf.float32))\n    \n    # Use hand with fewer NaN values\n    hand = tf.cond(right_nan > left_nan,\n                  lambda: left_hand,\n                  lambda: right_hand)\n    \n    # Normalize coordinates\n    mean = tf.reduce_mean(hand, axis=1, keepdims=True)\n    std = tf.math.reduce_std(hand, axis=1, keepdims=True)\n    std = tf.where(std < 1e-6, 1.0, std)\n    \n    hand = (hand - mean) / std\n    pose = (pose - mean) / std\n    \n    # Combine features\n    features = tf.concat([hand, pose], axis=1)\n    return features\n\ndef create_tf_dataset(file_paths, char_to_num, batch_size=BATCH_SIZE, is_training=False):\n    \"\"\"Create TensorFlow dataset from TFRecord files\"\"\"\n    \n    def parse_tfrecord(example_proto):\n        feature_description = {\n            'frames': tf.io.VarLenFeature(tf.float32),\n            'phrase': tf.io.FixedLenFeature([], tf.string)\n        }\n        parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n        \n        # Process frames\n        frames = tf.sparse.to_dense(parsed_features['frames'])\n        frames = tf.reshape(frames, [-1, len(SEL_COLS)])\n        frames = process_landmarks(frames)\n        \n        if is_training:\n            frames = augment_fn(frames)\n        \n        # Process phrase\n        phrase = parsed_features['phrase']\n        phrase = tf.strings.unicode_split(phrase, 'UTF-8')\n        phrase = table.lookup(phrase)\n        phrase = tf.pad(phrase, [[0, MAX_LEN - tf.shape(phrase)[0]]], constant_values=pad_token_idx)\n        \n        return frames, phrase\n    \n    # Create lookup table\n    table = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(\n            keys=list(char_to_num.keys()),\n            values=list(char_to_num.values()),\n        ),\n        default_value=pad_token_idx\n    )\n    \n    # Create dataset\n    dataset = tf.data.TFRecordDataset(file_paths, compression_type='GZIP')\n    \n    if is_training:\n        dataset = dataset.shuffle(10000)\n    \n    dataset = (dataset\n              .map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n              .batch(batch_size)\n              .prefetch(tf.data.AUTOTUNE))\n    \n    return dataset\n\n# Loss and metrics functions\ndef CTCLoss(labels, logits):\n    \"\"\"CTC Loss implementation\"\"\"\n    label_length = tf.reduce_sum(tf.cast(labels != pad_token_idx, tf.int32), axis=-1)\n    logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n    \n    loss = tf.nn.ctc_loss(\n        labels=tf.cast(labels, tf.int32),\n        logits=logits,\n        label_length=label_length,\n        logit_length=logit_length,\n        blank_index=pad_token_idx,\n        logits_time_major=False\n    )\n    return tf.reduce_mean(loss)\n\ndef calculate_levenshtein(predictions, targets, num_to_char):\n    \"\"\"Calculate normalized Levenshtein distance\"\"\"\n    distances = []\n    prediction_lengths = []\n    \n    for pred, target in zip(predictions, targets):\n        pred_text = ''.join([num_to_char.get(x, '') for x in pred if x not in [pad_token_idx, start_token_idx, end_token_idx]])\n        target_text = ''.join([num_to_char.get(x, '') for x in target if x not in [pad_token_idx, start_token_idx, end_token_idx]])\n        \n        distance = lev.distance(pred_text, target_text)\n        distances.append(distance)\n        prediction_lengths.append(len(target_text))\n    \n    return 1 - (sum(distances) / sum(prediction_lengths))\n\n# Learning rate schedule\ndef get_lr_schedule(initial_lr, warmup_epochs, total_epochs, steps_per_epoch):\n    warmup_steps = warmup_epochs * steps_per_epoch\n    total_steps = total_epochs * steps_per_epoch\n    \n    def lr_schedule(step):\n        # Linear warmup\n        if step < warmup_steps:\n            return initial_lr * (step / warmup_steps)\n        \n        # Cosine decay\n        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n        return initial_lr * 0.5 * (1 + tf.cos(math.pi * progress))\n    \n    return lr_schedule\n\n# Custom callbacks\nclass LevenshteinCallback(tf.keras.callbacks.Callback):\n    def __init__(self, validation_data, num_to_char):\n        super().__init__()\n        self.validation_data = validation_data\n        self.num_to_char = num_to_char\n        self.best_score = -float('inf')\n    \n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % 5 == 0:  # Calculate every 5 epochs to save time\n            predictions = []\n            targets = []\n            \n            for batch in self.validation_data:\n                pred = self.model(batch[0], training=False)\n                pred = tf.argmax(pred, axis=-1)\n                predictions.extend(pred.numpy())\n                targets.extend(batch[1].numpy())\n            \n            score = calculate_levenshtein(predictions, targets, self.num_to_char)\n            logs['levenshtein_score'] = score\n            \n            if score > self.best_score:\n                self.best_score = score\n                self.model.save_weights(f'best_model_epoch_{epoch}.h5')\n            \n            print(f'\\nLevenshtein Score: {score:.4f}')\n\n# Training function\ndef train_model(strategy, train_dataset, val_dataset, num_to_char, config):\n    with strategy.scope():\n        # Create model\n        model = ASLModel(\n            num_classes=len(num_to_char),\n            dim=config['dim'],\n            num_layers=config['num_layers'],\n            dropout_rate=config['dropout_rate']\n        )\n        \n        # Optimizer\n        optimizer = tfa.optimizers.RectifiedAdam(\n            learning_rate=config['initial_lr'],\n            weight_decay=config['weight_decay'],\n            sma_threshold=4.0\n        )\n        optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n        \n        # Compile model\n        model.compile(\n            optimizer=optimizer,\n            loss=CTCLoss,\n            #run_eagerly=True  # For debugging\n        )\n    \n    # Learning rate schedule\n    steps_per_epoch = len(train_dataset)\n    lr_schedule = get_lr_schedule(\n        config['initial_lr'],\n        config['warmup_epochs'],\n        config['epochs'],\n        steps_per_epoch\n    )\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.LearningRateScheduler(lr_schedule),\n        tf.keras.callbacks.ModelCheckpoint(\n            'checkpoint_epoch_{epoch}.h5',\n            save_weights_only=True,\n            save_freq='epoch'\n        ),\n        LevenshteinCallback(val_dataset, num_to_char),\n        tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=20,\n            restore_best_weights=True\n        )\n    ]\n    \n    # Train\n    history = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=config['epochs'],\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    return model, history\n\n# Main execution\ndef main():\n    # Configuration\n    config = {\n        'dim': 384,\n        'num_layers': 4,\n        'dropout_rate': 0.1,\n        'initial_lr': 1e-3,\n        'weight_decay': 0.05,\n        'warmup_epochs': 5,\n        'epochs': 100,\n        'batch_size': GLOBAL_BATCH_SIZE\n    }\n    \n    # Load character mapping\n    char_to_num, num_to_char = load_character_map('character_to_prediction_index.json')\n    \n    # Create TFRecord datasets\n    train_files = sorted(tf.io.gfile.glob('train/*.tfrecord'))\n    val_files = sorted(tf.io.gfile.glob('val/*.tfrecord'))\n    \n    train_dataset = create_tf_dataset(\n        train_files,\n        char_to_num,\n        batch_size=config['batch_size'],\n        is_training=True\n    )\n    \n    val_dataset = create_tf_dataset(\n        val_files,\n        char_to_num,\n        batch_size=config['batch_size'],\n        is_training=False\n    )\n    \n    # Train model\n    model, history = train_model(\n        strategy,\n        train_dataset,\n        val_dataset,\n        num_to_char,\n        config\n    )\n    \n    # Plot training history\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model Loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'])\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['levenshtein_score'])\n    plt.title('Levenshtein Score')\n    plt.ylabel('Score')\n    plt.xlabel('Epoch')\n    plt.show()\n    \n    # Save final model\n    model.save_weights('final_model.h5')\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:53:13.924161Z","iopub.execute_input":"2024-11-29T08:53:13.924471Z"}},"outputs":[],"execution_count":null}]}