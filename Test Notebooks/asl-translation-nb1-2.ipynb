{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T08:53:03.525496Z",
     "iopub.status.busy": "2024-11-29T08:53:03.524994Z",
     "iopub.status.idle": "2024-11-29T08:53:13.921986Z",
     "shell.execute_reply": "2024-11-29T08:53:13.920998Z",
     "shell.execute_reply.started": "2024-11-29T08:53:03.525440Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-levenshtein\n",
      "  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (16.1.0)\n",
      "Collecting Levenshtein==0.26.1 (from python-levenshtein)\n",
      "  Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.1->python-levenshtein)\n",
      "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow) (1.26.4)\n",
      "Downloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-levenshtein\n",
      "Successfully installed Levenshtein-0.26.1 python-levenshtein-0.26.1 rapidfuzz-3.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-levenshtein tqdm pyarrow wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-29T08:53:13.924471Z",
     "iopub.status.busy": "2024-11-29T08:53:13.924161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68 parquet files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8760c8f5c55e49e38312b2e2e82efcfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing parquet files:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 50700 sequences\n",
      "Found 68 parquet files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09ed3c3facd4c6fa0eba99b0f85b287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing parquet files:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 16508 sequences\n",
      "\n",
      "Starting training for 100 epochs\n",
      "==================================================\n",
      "\n",
      "Epoch 1/100\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/3998313249.py:782: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c874969b8c499ea3fda33748899db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/3998313249.py:919: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import Levenshtein\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "import wandb\n",
    "import sys\n",
    "\n",
    "wandb.login(key=\"afe8b8c0a3f1c1339a3daa9f619cb7c311218022\")\n",
    "wandb.init(project=\"asl-translation\")\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels: int = 3, output_dim: int = 52):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(input_channels, 64, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(64)\n",
    "        self.linear = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time, landmarks, channels]\n",
    "        B, T, L, C = x.shape\n",
    "        \n",
    "        # Reshape for 1D convolution over landmarks\n",
    "        x = x.permute(0, 1, 3, 2)  # [batch, time, channels, landmarks]\n",
    "        x = x.reshape(B * T, C, L)  # [batch*time, channels, landmarks]\n",
    "        \n",
    "        # Apply convolution\n",
    "        x = self.conv(x)  # [batch*time, 64, landmarks]\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Global average pooling over landmarks\n",
    "        x = x.mean(dim=2)  # [batch*time, 64]\n",
    "        \n",
    "        # Project to output dimension\n",
    "        x = self.linear(x)  # [batch*time, output_dim]\n",
    "        \n",
    "        # Reshape back to sequence\n",
    "        x = x.reshape(B, T, -1)  # [batch, time, output_dim]\n",
    "        \n",
    "        return x\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int = 384):\n",
    "        super().__init__()\n",
    "        head_dim = dim // 8  # Assuming 8 heads\n",
    "        half_head_dim = head_dim // 2\n",
    "        emb = math.log(10000) / (half_head_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_head_dim) * -emb)\n",
    "        pos = torch.arange(max_seq_len)\n",
    "        emb = pos[:, None] * emb[None, :]  # [max_seq_len, half_head_dim]\n",
    "        self.register_buffer('sin', emb.sin())\n",
    "        self.register_buffer('cos', emb.cos())\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        return self.sin[:seq_len], self.cos[:seq_len]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // num_heads\n",
    "        assert self.head_dim * num_heads == dim, \"dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def apply_rotary_pos_emb(self, q, k, sin, cos):\n",
    "        sin = sin.unsqueeze(0).unsqueeze(2)  # [1, seq_len, 1, head_dim//2]\n",
    "        cos = cos.unsqueeze(0).unsqueeze(2)  # [1, seq_len, 1, head_dim//2]\n",
    "        \n",
    "        # Separate half of head dim for rotation\n",
    "        q1, q2 = q.chunk(2, dim=-1)\n",
    "        k1, k2 = k.chunk(2, dim=-1)\n",
    "        \n",
    "        # Apply rotation using complementary pairs\n",
    "        q = torch.cat([\n",
    "            q1 * cos - q2 * sin,\n",
    "            q2 * cos + q1 * sin,\n",
    "        ], dim=-1)\n",
    "        \n",
    "        k = torch.cat([\n",
    "            k1 * cos - k2 * sin,\n",
    "            k2 * cos + k1 * sin,\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return q, k\n",
    "\n",
    "    def forward(self, x: torch.Tensor, sin: torch.Tensor, cos: torch.Tensor, \n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        B, L, D = x.shape\n",
    "        \n",
    "        # Project inputs\n",
    "        q = self.q_proj(x).reshape(B, L, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, L, self.num_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, L, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Apply rotary embeddings\n",
    "        q, k = self.apply_rotary_pos_emb(q, k, sin, cos)\n",
    "        \n",
    "        # Reshape for attention\n",
    "        q = q.transpose(1, 2)  # [B, H, L, D/H]\n",
    "        k = k.transpose(1, 2)  # [B, H, L, D/H]\n",
    "        v = v.transpose(1, 2)  # [B, H, L, D/H]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scale = self.head_dim ** -0.5\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Expand mask for attention heads\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, L]\n",
    "            attn = attn.masked_fill(~mask, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Combine with values\n",
    "        out = torch.matmul(attn, v)  # [B, H, L, D/H]\n",
    "        out = out.transpose(1, 2).contiguous()  # [B, L, H, D/H]\n",
    "        out = out.reshape(B, L, D)  # [B, L, D]\n",
    "        \n",
    "        return self.out_proj(out)\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.mhsa = MultiHeadAttention(dim, num_heads, dropout)\n",
    "        \n",
    "        # Convolution module\n",
    "        self.conv_norm = nn.LayerNorm(dim)\n",
    "        self.conv1 = nn.Conv1d(dim, dim*2, 1)\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "        self.depthwise_conv = nn.Conv1d(dim, dim, 3, padding=1, groups=dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(dim)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.pointwise_conv = nn.Conv1d(dim, dim, 1)\n",
    "        self.conv_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feed forward module\n",
    "        self.ff_norm = nn.LayerNorm(dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim*4, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, sin: torch.Tensor, cos: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Self attention\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.mhsa(x, sin, cos, mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x * self.scale\n",
    "        \n",
    "        # Convolution module\n",
    "        residual = x\n",
    "        x = self.conv_norm(x)\n",
    "        x = x.transpose(1, 2)  # [B, C, T]\n",
    "        x = self.conv1(x)\n",
    "        x = self.glu(x)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        x = self.conv_dropout(x)\n",
    "        x = x.transpose(1, 2)  # [B, T, C]\n",
    "        x = residual + x * self.scale\n",
    "        \n",
    "        # Feed forward\n",
    "        residual = x\n",
    "        x = self.ff_norm(x)\n",
    "        x = self.ff(x)\n",
    "        x = residual + x * self.scale\n",
    "        \n",
    "        return x\n",
    "\n",
    "class SqueezeformerBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.mhsa = MultiHeadAttention(dim, num_heads, dropout)\n",
    "        \n",
    "        # Feed forward modules\n",
    "        self.ff1_norm = nn.LayerNorm(dim)\n",
    "        self.ff1 = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim*4, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Convolution module\n",
    "        self.conv_norm = nn.LayerNorm(dim)\n",
    "        self.conv1 = nn.Conv1d(dim, dim*2, 1)\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "        self.depthwise_conv = nn.Conv1d(dim, dim, 3, padding=1, groups=dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(dim)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.pointwise_conv = nn.Conv1d(dim, dim, 1)\n",
    "        self.conv_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feed forward module 2\n",
    "        self.ff2_norm = nn.LayerNorm(dim)\n",
    "        self.ff2 = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim*4, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, sin: torch.Tensor, cos: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # First feed forward\n",
    "        residual = x\n",
    "        x = self.ff1_norm(x)\n",
    "        x = self.ff1(x)\n",
    "        x = residual + x * self.scale\n",
    "        \n",
    "        # Self attention\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.mhsa(x, sin, cos, mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x * self.scale\n",
    "        \n",
    "        # Convolution module\n",
    "        residual = x\n",
    "        x = self.conv_norm(x)\n",
    "        x = x.transpose(1, 2)  # [B, C, T]\n",
    "        x = self.conv1(x)\n",
    "        x = self.glu(x)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        x = self.conv_dropout(x)\n",
    "        x = x.transpose(1, 2)  # [B, T, C]\n",
    "        x = residual + x * self.scale\n",
    "        \n",
    "        # Second feed forward\n",
    "        residual = x\n",
    "        x = self.ff2_norm(x)\n",
    "        x = self.ff2(x)\n",
    "        x = residual + x * self.scale\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ASLTranslationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_landmarks: int = 130,\n",
    "        feature_dim: int = 208,\n",
    "        num_classes: int = 59,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extractors for different landmark types\n",
    "        self.face_extractor = FeatureExtractor(3, 52)\n",
    "        self.pose_extractor = FeatureExtractor(3, 52)\n",
    "        self.left_hand_extractor = FeatureExtractor(3, 52)\n",
    "        self.right_hand_extractor = FeatureExtractor(3, 52)\n",
    "        \n",
    "        # Target embedding\n",
    "        self.target_embedding = nn.Embedding(num_classes, feature_dim)\n",
    "        self.pos_embedding = RotaryPositionalEmbedding(feature_dim)\n",
    "        \n",
    "        # Parallel encoders\n",
    "        self.conformer_layers = nn.ModuleList([\n",
    "            ConformerBlock(feature_dim, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.squeezeformer_layers = nn.ModuleList([\n",
    "            SqueezeformerBlock(feature_dim, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=feature_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=feature_dim*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n",
    "        \n",
    "        # Output layers\n",
    "        self.confidence_head = nn.Linear(feature_dim, 1)\n",
    "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        tgt: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, T, L, C = x.shape\n",
    "        \n",
    "        # Extract features\n",
    "        face = x[:, :, :76]\n",
    "        pose = x[:, :, 76:88]\n",
    "        left_hand = x[:, :, 88:109]\n",
    "        right_hand = x[:, :, 109:]\n",
    "        \n",
    "        face_feats = self.face_extractor(face)\n",
    "        pose_feats = self.pose_extractor(pose)\n",
    "        left_hand_feats = self.left_hand_extractor(left_hand)\n",
    "        right_hand_feats = self.right_hand_extractor(right_hand)\n",
    "        \n",
    "        features = torch.cat(\n",
    "            [face_feats, pose_feats, left_hand_feats, right_hand_feats],\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        # Process features through encoder\n",
    "        sin, cos = self.pos_embedding(features)\n",
    "        \n",
    "        # (Continuing the ASLTranslationModel forward method)\n",
    "        \n",
    "        conformer_out = features\n",
    "        squeezeformer_out = features\n",
    "        \n",
    "        # Convert mask to padding mask for encoder if provided\n",
    "        encoder_padding_mask = None\n",
    "        if mask is not None:\n",
    "            encoder_padding_mask = mask  # [B, T]\n",
    "        \n",
    "        for conf_layer, squeeze_layer in zip(\n",
    "            self.conformer_layers,\n",
    "            self.squeezeformer_layers\n",
    "        ):\n",
    "            conformer_out = conf_layer(conformer_out, sin, cos, encoder_padding_mask)\n",
    "            squeezeformer_out = squeeze_layer(squeezeformer_out, sin, cos, encoder_padding_mask)\n",
    "        \n",
    "        encoder_out = conformer_out + squeezeformer_out\n",
    "        confidence = self.confidence_head(encoder_out[:, 0]).squeeze(-1)\n",
    "        \n",
    "        if tgt is not None:\n",
    "            # Process target sequence\n",
    "            tgt_embedded = self.target_embedding(tgt)  # [B, tgt_len, dim]\n",
    "            tgt_embedded = self.dropout(tgt_embedded)\n",
    "            \n",
    "            # Create causal mask for target self-attention\n",
    "            tgt_len = tgt.size(1)\n",
    "            tgt_mask = self.generate_square_subsequent_mask(tgt_len).to(x.device)\n",
    "            \n",
    "            # Create memory padding mask for encoder-decoder attention\n",
    "            memory_padding_mask = None\n",
    "            if encoder_padding_mask is not None:\n",
    "                memory_padding_mask = ~encoder_padding_mask  # Convert to padding mask format\n",
    "            \n",
    "            # Decoder forward pass\n",
    "            decoder_out = self.decoder(\n",
    "                tgt_embedded,\n",
    "                encoder_out,\n",
    "                tgt_mask=tgt_mask,\n",
    "                memory_key_padding_mask=memory_padding_mask\n",
    "            )\n",
    "            output = self.classifier(decoder_out)\n",
    "        else:\n",
    "            output = self.classifier(encoder_out)\n",
    "        \n",
    "        return output, confidence\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "        \"\"\"Generate causal mask for decoder self-attention\"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        mask = mask.masked_fill(mask == 0, float(0.0))\n",
    "        return mask\n",
    "\n",
    "def train_step(\n",
    "    model: ASLTranslationModel,\n",
    "    batch: dict,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: str = 'cuda',\n",
    "    grad_scaler = None\n",
    ") -> float:\n",
    "    \"\"\"Perform one training step\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Move batch to device\n",
    "    landmarks = batch['landmarks'].to(device)\n",
    "    tokens = batch['tokens'].to(device)\n",
    "    mask = batch['mask'].to(device) if 'mask' in batch else None\n",
    "    \n",
    "    # Forward pass with mixed precision\n",
    "    with torch.cuda.amp.autocast():\n",
    "        pred, confidence = model(landmarks, mask, tokens[:, :-1])\n",
    "        \n",
    "        # Calculate confidence target (normalized Levenshtein distance)\n",
    "        with torch.no_grad():\n",
    "            confidence_target = torch.tensor([\n",
    "                1 - Levenshtein.distance(\n",
    "                    model.tokenizer.decode(p.argmax(-1).cpu()),\n",
    "                    true_text\n",
    "                ) / max(len(true_text), 1)\n",
    "                for p, true_text in zip(pred, batch['phrase'])\n",
    "            ]).to(device)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(pred, tokens[:, 1:], confidence, confidence_target)\n",
    "    \n",
    "    # Backward pass with gradient scaling\n",
    "    if grad_scaler is not None:\n",
    "        grad_scaler.scale(loss).backward()\n",
    "        grad_scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        grad_scaler.step(optimizer)\n",
    "        grad_scaler.update()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "class ASLTranslationLoss(nn.Module):\n",
    "    \"\"\"Combined loss function for sequence prediction and confidence score\"\"\"\n",
    "    def __init__(self, pad_idx: int = 0):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        pred: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        confidence: torch.Tensor,\n",
    "        confidence_target: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # Main sequence prediction loss\n",
    "        seq_loss = self.criterion(\n",
    "            pred.reshape(-1, pred.size(-1)),\n",
    "            target.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # Confidence prediction loss (MSE)\n",
    "        conf_loss = F.mse_loss(confidence, confidence_target)\n",
    "        \n",
    "        # Combine losses\n",
    "        return seq_loss + 0.1 * conf_loss\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask\n",
    "\n",
    "class ASLTokenizer:\n",
    "    \"\"\"Tokenizer for ASL fingerspelling sequences\"\"\"\n",
    "    def __init__(self, vocab_path: str):\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.char_to_idx = json.load(f)\n",
    "            \n",
    "        self.idx_to_char = {v: k for k, v in self.char_to_idx.items()}\n",
    "        self.vocab_size = len(self.char_to_idx)\n",
    "        self.pad_token = 0\n",
    "        self.sos_token = 1\n",
    "        self.eos_token = 2\n",
    "        \n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Convert text to token indices\"\"\"\n",
    "        tokens = [self.sos_token]\n",
    "        for char in text:\n",
    "            tokens.append(self.char_to_idx.get(char, self.pad_token))\n",
    "        tokens.append(self.eos_token)\n",
    "        return torch.tensor(tokens)\n",
    "    \n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        \"\"\"Convert token indices to text\"\"\"\n",
    "        text = []\n",
    "        for token in tokens:\n",
    "            if token.item() == self.eos_token:\n",
    "                break\n",
    "            if token.item() not in [self.pad_token, self.sos_token]:\n",
    "                text.append(self.idx_to_char[token.item()])\n",
    "        return ''.join(text)\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "class ASLDataset(Dataset):\n",
    "    \"\"\"Dataset for ASL fingerspelling using pre-processed TFRecords\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        tf_records: List[str],\n",
    "        tokenizer,\n",
    "        max_len: int = 384,\n",
    "        augment: bool = True,\n",
    "        mode: str = 'train'\n",
    "    ):\n",
    "        self.tf_records = tf_records\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.augment = augment\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Create schema for TFRecord parsing\n",
    "        self.feature_description = {\n",
    "            \"frames\": tf.io.VarLenFeature(dtype=tf.float32),\n",
    "            \"phrase\": tf.io.VarLenFeature(dtype=tf.float32)\n",
    "        }\n",
    "        \n",
    "        # Pre-load data for faster access\n",
    "        print(f\"Loading {mode} dataset from TFRecords...\")\n",
    "        self.data = []\n",
    "        for record in tqdm(tf_records, desc=f\"Loading {mode} dataset\"):\n",
    "            dataset = tf.data.TFRecordDataset([record], compression_type='GZIP')\n",
    "            for example in dataset:\n",
    "                self.data.append(self._process_example(example))\n",
    "    \n",
    "    def _process_example(self, example):\n",
    "        \"\"\"Process a single TFRecord example\"\"\"\n",
    "        # Parse TFRecord\n",
    "        features = tf.io.parse_single_example(example, self.feature_description)\n",
    "        \n",
    "        # Extract frames and phrase\n",
    "        frames = tf.sparse.to_dense(features[\"frames\"])\n",
    "        phrase = tf.cast(tf.sparse.to_dense(features[\"phrase\"]), tf.int32)\n",
    "        \n",
    "        # Reshape frames to [time, landmarks, 3]\n",
    "        frames = tf.transpose(tf.reshape(frames, (-1, 2, int(len(SEL_COLS)/2))), [0, 2, 1])\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        frames = torch.from_numpy(frames.numpy()).float()\n",
    "        \n",
    "        # Apply augmentation if enabled\n",
    "        if self.augment and random.random() < 0.2:\n",
    "            frames = self.augment_landmarks(frames)\n",
    "        \n",
    "        # Handle sequence length\n",
    "        T = frames.shape[0]\n",
    "        if T > self.max_len:\n",
    "            indices = torch.linspace(0, T-1, self.max_len).long()\n",
    "            frames = frames[indices]\n",
    "        else:\n",
    "            pad_len = self.max_len - T\n",
    "            frames = F.pad(frames, (0, 0, 0, 0, 0, pad_len))\n",
    "        \n",
    "        # Get phrase text\n",
    "        phrase_text = \"\".join([num_to_char.get(x, \"\") for x in phrase.numpy()])\n",
    "        tokens = self.tokenizer.encode(phrase_text)\n",
    "        \n",
    "        return {\n",
    "            'landmarks': frames,\n",
    "            'tokens': tokens,\n",
    "            'phrase': phrase_text,\n",
    "            'length': torch.tensor(min(T, self.max_len))\n",
    "        }\n",
    "    \n",
    "    def augment_landmarks(self, landmarks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply augmentations to landmark sequence\"\"\"\n",
    "        T = landmarks.shape[0]\n",
    "        \n",
    "        # Time augmentations\n",
    "        if random.random() < 0.8:\n",
    "            scale = random.uniform(0.8, 1.2)\n",
    "            new_T = int(T * scale)\n",
    "            if new_T > 0:\n",
    "                indices = torch.linspace(0, T-1, new_T).long()\n",
    "                landmarks = landmarks[indices]\n",
    "                T = new_T\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            landmarks = spatial_random_affine(landmarks)\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            landmarks = temporal_mask(landmarks)\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            landmarks = spatial_mask(landmarks)\n",
    "        \n",
    "        return landmarks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def get_landmarks(self, sequence_id: str) -> np.ndarray:\n",
    "        \"\"\"Load landmarks for a specific sequence from parquet file\"\"\"\n",
    "        parquet_file = self.sequence_to_file[sequence_id]\n",
    "        \n",
    "        # Read the specific sequence from parquet file\n",
    "        table = pq.read_table(\n",
    "            parquet_file,\n",
    "            filters=[('sequence_id', '=', sequence_id)]\n",
    "        )\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract landmark columns (excluding sequence_id and frame)\n",
    "        landmark_cols = [col for col in df.columns if col not in ['sequence_id', 'frame']]\n",
    "        landmarks = df[landmark_cols].values\n",
    "        \n",
    "        # Reshape landmarks to (frames, landmarks, 3)\n",
    "        num_landmarks = len(landmark_cols) // 3\n",
    "        landmarks = landmarks.reshape(-1, num_landmarks, 3)\n",
    "        \n",
    "        return landmarks\n",
    "        \n",
    "    def augment_landmarks(self, landmarks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply augmentations to landmark sequence with safety checks\"\"\"\n",
    "        T = landmarks.shape[0]  # sequence length\n",
    "        \n",
    "        # Time augmentations\n",
    "        if random.random() < 0.8:\n",
    "            # Random resize along time axis\n",
    "            scale = random.uniform(0.8, 1.2)\n",
    "            new_T = int(T * scale)\n",
    "            if new_T > 0:  # Only resize if new length is valid\n",
    "                indices = torch.linspace(0, T-1, new_T).long()\n",
    "                landmarks = landmarks[indices]\n",
    "                T = new_T  # Update sequence length\n",
    "        \n",
    "        if random.random() < 0.5 and T > 1:\n",
    "            # Random time shift\n",
    "            shift = random.randint(-min(5, T//2), min(5, T//2))\n",
    "            landmarks = torch.roll(landmarks, shift, dims=0)\n",
    "        \n",
    "        # Spatial augmentations\n",
    "        if random.random() < 0.8:\n",
    "            # Random spatial affine\n",
    "            angle = random.uniform(-30, 30)\n",
    "            scale = random.uniform(0.8, 1.2)\n",
    "            shear = random.uniform(-0.2, 0.2)\n",
    "            translate = (random.uniform(-0.1, 0.1), random.uniform(-0.1, 0.1))\n",
    "            \n",
    "            theta = torch.tensor([\n",
    "                [scale * math.cos(angle), -scale * math.sin(angle) + shear, translate[0]],\n",
    "                [scale * math.sin(angle) + shear, scale * math.cos(angle), translate[1]]\n",
    "            ]).float()[None]  # Add batch dimension\n",
    "            \n",
    "            # Handle each frame separately\n",
    "            landmarks_2d = landmarks[..., :2]  # Only x,y coordinates\n",
    "            \n",
    "            # Process each frame\n",
    "            transformed_frames = []\n",
    "            for t in range(T):\n",
    "                frame = landmarks_2d[t:t+1]  # Add batch dimension\n",
    "                \n",
    "                grid = F.affine_grid(\n",
    "                    theta,\n",
    "                    size=(1, 1, frame.shape[1], 2),\n",
    "                    align_corners=False\n",
    "                )\n",
    "                \n",
    "                transformed = F.grid_sample(\n",
    "                    frame[:, None],  # Add channel dimension\n",
    "                    grid,\n",
    "                    align_corners=False\n",
    "                )\n",
    "                transformed_frames.append(transformed[:, 0])  # Remove channel dimension\n",
    "            \n",
    "            # Stack frames back together\n",
    "            landmarks_2d = torch.cat(transformed_frames, dim=0)\n",
    "            landmarks[..., :2] = landmarks_2d\n",
    "        \n",
    "        # Landmark dropping\n",
    "        if random.random() < 0.5 and T >= 20:  # Only apply to sequences long enough\n",
    "            # Randomly drop fingers\n",
    "            num_fingers = random.randint(1, 3)\n",
    "            num_windows = random.randint(1, 2)\n",
    "            \n",
    "            for _ in range(num_windows):\n",
    "                if T <= 20:  # Skip if sequence too short\n",
    "                    break\n",
    "                \n",
    "                window_size = min(random.randint(5, 10), T-1)\n",
    "                window_start = random.randint(0, T - window_size)\n",
    "                window_end = window_start + window_size\n",
    "                \n",
    "                for _ in range(num_fingers):\n",
    "                    finger_start = random.randint(88, 126)\n",
    "                    landmarks[window_start:window_end, finger_start:finger_start+4] = 0\n",
    "        \n",
    "        if random.random() < 0.3:\n",
    "            # Drop face or pose landmarks\n",
    "            if random.random() < 0.5:\n",
    "                landmarks[:, :76] = 0  # Face\n",
    "            else:\n",
    "                landmarks[:, 76:88] = 0  # Pose\n",
    "        \n",
    "        if random.random() < 0.05:\n",
    "            # Drop all hand landmarks\n",
    "            landmarks[:, 88:] = 0\n",
    "        \n",
    "        return landmarks\n",
    "    \n",
    "    def normalize_landmarks(self, landmarks: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"Normalize landmarks with mean and std\"\"\"\n",
    "        # Convert to tensor\n",
    "        landmarks = torch.from_numpy(landmarks).float()\n",
    "        \n",
    "        # Calculate mean and std per dimension\n",
    "        mean = landmarks.mean(dim=0, keepdim=True)\n",
    "        std = landmarks.std(dim=0, keepdim=True)\n",
    "        std[std == 0] = 1\n",
    "        \n",
    "        # Normalize\n",
    "        landmarks = (landmarks - mean) / std\n",
    "        \n",
    "        # Fill nans with zeros\n",
    "        landmarks = torch.nan_to_num(landmarks)\n",
    "        \n",
    "        return landmarks\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load landmarks\n",
    "        landmarks = self.get_landmarks(row['sequence_id'])\n",
    "        landmarks = self.normalize_landmarks(landmarks)\n",
    "        \n",
    "        # Only apply augmentations to 20% of the data when augment flag is True\n",
    "        if self.augment and random.random() < 0.2:\n",
    "            landmarks = self.augment_landmarks(landmarks)\n",
    "            \n",
    "        # Pad or resize sequence to max_len\n",
    "        T = landmarks.shape[0]\n",
    "        if T > self.max_len:\n",
    "            # Resize if too long using linear interpolation\n",
    "            indices = torch.linspace(0, T-1, self.max_len).long()\n",
    "            landmarks = landmarks[indices]\n",
    "        else:\n",
    "            # Pad if too short\n",
    "            pad_len = self.max_len - T\n",
    "            landmarks = F.pad(landmarks, (0, 0, 0, 0, 0, pad_len))\n",
    "            \n",
    "        # Tokenize phrase\n",
    "        tokens = self.tokenizer.encode(row['phrase'])\n",
    "        \n",
    "        return {\n",
    "            'landmarks': landmarks,\n",
    "            'tokens': tokens,\n",
    "            'phrase': row['phrase'],\n",
    "            'length': torch.tensor(T)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Custom collate function for batching\"\"\"\n",
    "    # Pad sequences to max length in batch\n",
    "    max_token_len = max(item['tokens'].size(0) for item in batch)\n",
    "    \n",
    "    # Prepare tensors\n",
    "    landmarks = torch.stack([item['landmarks'] for item in batch])\n",
    "    tokens = torch.stack([\n",
    "        F.pad(item['tokens'], (0, max_token_len - item['tokens'].size(0)), value=0)\n",
    "        for item in batch\n",
    "    ])\n",
    "    lengths = torch.stack([item['length'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'landmarks': landmarks,\n",
    "        'tokens': tokens,\n",
    "        'phrase': [item['phrase'] for item in batch],\n",
    "        'length': lengths\n",
    "    }\n",
    "    \n",
    "class Trainer:\n",
    "    \"\"\"Training controller for ASL Translation model with WandB logging\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        tokenizer: ASLTokenizer,\n",
    "        learning_rate: float = 0.0045,\n",
    "        weight_decay: float = 0.08,\n",
    "        warmup_epochs: int = 1,\n",
    "        max_epochs: int = 2,\n",
    "        device: str = 'cuda',\n",
    "        wandb_config: dict = None\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_epochs = max_epochs\n",
    "        self.wandb_config = wandb_config\n",
    "        \n",
    "        # Initialize WandB\n",
    "        # Initialize WandB\n",
    "        if wandb_config:\n",
    "            wandb.init(\n",
    "                project=wandb_config['project'],\n",
    "                name=wandb_config['run_name'],\n",
    "                config={\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'weight_decay': weight_decay,\n",
    "                    'warmup_epochs': warmup_epochs,\n",
    "                    'max_epochs': max_epochs,\n",
    "                    'batch_size': train_loader.batch_size,\n",
    "                    'architecture': 'Conformer-Squeezeformer'\n",
    "                }\n",
    "            )\n",
    "            # Watch model gradients\n",
    "            wandb.watch(model, log_freq=100)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.num_training_steps = len(train_loader) * max_epochs\n",
    "        self.num_warmup_steps = len(train_loader) * warmup_epochs\n",
    "        self.scheduler = self.get_scheduler()\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = ASLTranslationLoss()\n",
    "        \n",
    "        # Gradient scaler for mixed precision training\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "    def get_scheduler(self):\n",
    "        return torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.optimizer.param_groups[0]['lr'],\n",
    "            total_steps=self.num_training_steps,\n",
    "            pct_start=self.num_warmup_steps / self.num_training_steps,\n",
    "            anneal_strategy='cos',\n",
    "            cycle_momentum=False\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self) -> float:\n",
    "        \"\"\"Train for one epoch with proper progress tracking and WandB logging\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = len(self.train_loader)\n",
    "        \n",
    "        progress_bar = tqdm(\n",
    "            self.train_loader,\n",
    "            desc='Training',\n",
    "            leave=True,\n",
    "            position=0,\n",
    "            dynamic_ncols=True\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                try:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    # Move batch to device\n",
    "                    landmarks = batch['landmarks'].to(self.device)\n",
    "                    tokens = batch['tokens'].to(self.device)\n",
    "                    lengths = batch['length'].to(self.device)\n",
    "                    \n",
    "                    # Create mask based on sequence lengths\n",
    "                    seq_length = landmarks.size(1)\n",
    "                    position_indices = torch.arange(seq_length, device=self.device)[None, :]\n",
    "                    mask = position_indices < lengths[:, None]\n",
    "                    \n",
    "                    try:\n",
    "                        with autocast():\n",
    "                            pred, confidence = self.model(landmarks, mask, tokens[:, :-1])\n",
    "                            \n",
    "                            # Calculate confidence target\n",
    "                            with torch.no_grad():\n",
    "                                confidence_target = torch.tensor([\n",
    "                                    1 - Levenshtein.distance(\n",
    "                                        self.tokenizer.decode(p.argmax(-1).cpu()),\n",
    "                                        true_text\n",
    "                                    ) / max(len(true_text), 1)\n",
    "                                    for p, true_text in zip(pred, batch['phrase'])\n",
    "                                ], device=self.device)\n",
    "                            \n",
    "                            loss = self.criterion(pred, tokens[:, 1:], confidence, confidence_target)\n",
    "                        \n",
    "                        # Backward pass with gradient scaling\n",
    "                        self.scaler.scale(loss).backward()\n",
    "                        self.scaler.unscale_(self.optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                        self.scheduler.step()\n",
    "                        \n",
    "                        # Update metrics\n",
    "                        total_loss += loss.item()\n",
    "                        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                        \n",
    "                        # Log to WandB\n",
    "                        if self.wandb_config and batch_idx % 10 == 0:  # Log every 10 batches\n",
    "                            wandb.log({\n",
    "                                'batch_loss': loss.item(),\n",
    "                                'learning_rate': current_lr,\n",
    "                                'batch_confidence': confidence.mean().item(),\n",
    "                                'batch_confidence_target': confidence_target.mean().item()\n",
    "                            })\n",
    "                        \n",
    "                        # Update progress bar\n",
    "                        progress_bar.set_postfix({\n",
    "                            'batch_loss': f'{loss.item():.4f}',\n",
    "                            'avg_loss': f'{total_loss/(batch_idx+1):.4f}',\n",
    "                            'lr': f'{current_lr:.2e}',\n",
    "                            'batch': f'{batch_idx+1}/{num_batches}'\n",
    "                        })\n",
    "                        \n",
    "                        if batch_idx % 10 == 0:\n",
    "                            progress_bar.refresh()\n",
    "                    \n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"\\nError in forward/backward pass: {str(e)}\")\n",
    "                        if \"out of memory\" in str(e):\n",
    "                            if torch.cuda.is_available():\n",
    "                                torch.cuda.empty_cache()\n",
    "                        continue\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing batch {batch_idx}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted by user\")\n",
    "            return total_loss / (batch_idx + 1) if batch_idx > 0 else float('inf')\n",
    "        \n",
    "        finally:\n",
    "            progress_bar.close()\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self) -> Tuple[float, float]:\n",
    "        \"\"\"Validate model and compute metrics\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "        confidence_scores = []\n",
    "        \n",
    "        for batch in tqdm(self.val_loader, desc='Validating'):\n",
    "            # Move batch to device\n",
    "            landmarks = batch['landmarks'].to(self.device)\n",
    "            tokens = batch['tokens'].to(self.device)\n",
    "            lengths = batch['length'].to(self.device)\n",
    "            \n",
    "            # Create mask based on sequence lengths\n",
    "            mask = torch.arange(landmarks.size(1))[None, :] < lengths[:, None]\n",
    "            mask = mask.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred, confidence = self.model(landmarks, mask)\n",
    "            confidence_scores.extend(confidence.cpu().tolist())\n",
    "            \n",
    "            # Decode predictions\n",
    "            pred_texts = [\n",
    "                self.tokenizer.decode(p.argmax(-1))\n",
    "                for p in pred\n",
    "            ]\n",
    "            predictions.extend(pred_texts)\n",
    "            ground_truth.extend(batch['phrase'])\n",
    "            \n",
    "            # Calculate loss\n",
    "            confidence_target = torch.tensor([\n",
    "                1 - Levenshtein.distance(pred_text, true_text) / max(len(true_text), 1)\n",
    "                for pred_text, true_text in zip(pred_texts, batch['phrase'])\n",
    "            ]).to(self.device)\n",
    "            \n",
    "            loss = self.criterion(pred, tokens[:, 1:], confidence, confidence_target)\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        \n",
    "        # Calculate normalized Levenshtein distance\n",
    "        distances = [\n",
    "            1 - Levenshtein.distance(pred, true) / max(len(pred), len(true))\n",
    "            for pred, true in zip(predictions, ground_truth)\n",
    "        ]\n",
    "        avg_score = sum(distances) / len(distances)\n",
    "        \n",
    "        # Log validation examples to WandB\n",
    "        if self.wandb_config:\n",
    "            # Log validation metrics\n",
    "            wandb.log({\n",
    "                'val_loss': avg_loss,\n",
    "                'val_score': avg_score,\n",
    "                'val_confidence_mean': np.mean(confidence_scores),\n",
    "                'val_confidence_std': np.std(confidence_scores)\n",
    "            })\n",
    "            \n",
    "            # Log prediction examples\n",
    "            examples = []\n",
    "            for i in range(min(10, len(predictions))):\n",
    "                examples.append(wandb.Table(\n",
    "                    columns=['Ground Truth', 'Prediction', 'Score'],\n",
    "                    data=[[ground_truth[i], predictions[i], distances[i]]]\n",
    "                ))\n",
    "            wandb.log({\"prediction_examples\": examples})\n",
    "        \n",
    "        return avg_loss, avg_score\n",
    "\n",
    "    def train(self, save_dir: str):\n",
    "        \"\"\"Train model with improved logging and error handling\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nStarting training for {self.max_epochs} epochs\")\n",
    "        print(f\"Training device: {self.device}\")\n",
    "        print(f\"Number of training batches: {len(self.train_loader)}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        best_val_score = float('-inf')\n",
    "        \n",
    "        try:\n",
    "            for epoch in range(self.max_epochs):\n",
    "                epoch_start_time = time.time()\n",
    "                \n",
    "                print(f\"\\nEpoch {epoch + 1}/{self.max_epochs}\")\n",
    "                print(\"-\" * 30)\n",
    "                \n",
    "                # Train epoch\n",
    "                train_loss = self.train_epoch()\n",
    "                epoch_time = time.time() - epoch_start_time\n",
    "                \n",
    "                # Print and log metrics\n",
    "                print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "                print(f\"Training Loss: {train_loss:.4f}\")\n",
    "                print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "                print(f\"Epoch Time: {epoch_time:.1f}s\")\n",
    "                \n",
    "                if self.wandb_config:\n",
    "                    wandb.log({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'train_loss': train_loss,\n",
    "                        'epoch_time': epoch_time\n",
    "                    })\n",
    "                \n",
    "                # Validate and save checkpoint periodically\n",
    "                if (epoch + 1) % 5 == 0:\n",
    "                    print(\"\\nRunning validation...\")\n",
    "                    val_loss, val_score = self.validate()\n",
    "                    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "                    print(f\"Validation Score: {val_score:.4f}\")\n",
    "                    \n",
    "                    # Save best model\n",
    "                    if val_score > best_val_score:\n",
    "                        best_val_score = val_score\n",
    "                        checkpoint_path = os.path.join(save_dir, 'best_model.pt')\n",
    "                        torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                            'val_score': val_score,\n",
    "                        }, checkpoint_path)\n",
    "                        print(f\"✓ Saved new best model (score: {val_score:.4f})\")\n",
    "                        \n",
    "                        if self.wandb_config:\n",
    "                            wandb.log({'best_val_score': val_score})\n",
    "                \n",
    "                # Save periodic checkpoint\n",
    "                if (epoch + 1) % 40 == 0:\n",
    "                    checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                    }, checkpoint_path)\n",
    "                    print(f\"✓ Saved checkpoint at epoch {epoch+1}\")\n",
    "                \n",
    "                # Force flush stdout\n",
    "                sys.stdout.flush()\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted by user\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nTraining failed with error: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        finally:\n",
    "            # Save final model\n",
    "            final_path = os.path.join(save_dir, 'final_model.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                'best_score': best_val_score,\n",
    "            }, final_path)\n",
    "            print(f\"\\n✓ Saved final model\")\n",
    "            \n",
    "            # Finish WandB run\n",
    "            if self.wandb_config:\n",
    "                wandb.finish()\n",
    "\n",
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Configuration\n",
    "    config = {\n",
    "        'data_dir': '/kaggle/input/asl-fingerspelling/train_landmarks',\n",
    "        'metadata_path': '/kaggle/input/asl-fingerspelling/train.csv',\n",
    "        'vocab_path': '/kaggle/input/asl-fingerspelling/character_to_prediction_index.json',\n",
    "        'save_dir': '/kaggle/working/models',\n",
    "        'batch_size': 64,\n",
    "        'max_len': 384,\n",
    "        'num_workers': 2,\n",
    "        'learning_rate': 0.0045,\n",
    "        'weight_decay': 0.08,\n",
    "        'warmup_epochs': 1,\n",
    "        'max_epochs': 2,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'wandb_project': 'asl-translation',\n",
    "        'run_name': f'asl-translation-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    }\n",
    "\n",
    "    # WandB configuration\n",
    "    wandb_config = {\n",
    "        'project': config['wandb_project'],\n",
    "        'run_name': config['run_name']\n",
    "    }\n",
    "\n",
    "    print(\"\\nInitializing training...\")\n",
    "    print(f\"Device: {config['device']}\")\n",
    "    \n",
    "    # Make sure save directory exists\n",
    "    os.makedirs(config['save_dir'], exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Initialize tokenizer\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = ASLTokenizer(config['vocab_path'])\n",
    "        \n",
    "        # Create datasets\n",
    "        print(\"\\nCreating datasets...\")\n",
    "        train_dataset = ASLDataset(\n",
    "            config['data_dir'],\n",
    "            config['metadata_path'],\n",
    "            tokenizer,\n",
    "            max_len=config['max_len'],\n",
    "            augment=True,\n",
    "            mode='train'\n",
    "        )\n",
    "        \n",
    "        val_dataset = ASLDataset(\n",
    "            config['data_dir'],\n",
    "            config['metadata_path'],\n",
    "            tokenizer,\n",
    "            max_len=config['max_len'],\n",
    "            augment=False,\n",
    "            mode='val'\n",
    "        )\n",
    "        \n",
    "        print(f\"Training samples: {len(train_dataset)}\")\n",
    "        print(f\"Validation samples: {len(val_dataset)}\")\n",
    "        \n",
    "        # Create dataloaders\n",
    "        print(\"\\nCreating dataloaders...\")\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=config['num_workers'],\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=config['num_workers'],\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        print(\"\\nInitializing model...\")\n",
    "        model = ASLTranslationModel(\n",
    "            num_landmarks=130,\n",
    "            feature_dim=208,\n",
    "            num_classes=len(tokenizer.char_to_idx),\n",
    "            num_layers=2,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "            model = nn.DataParallel(model)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Initialize trainer\n",
    "        print(\"\\nInitializing trainer...\")\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            tokenizer=tokenizer,\n",
    "            learning_rate=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "            warmup_epochs=config['warmup_epochs'],\n",
    "            max_epochs=config['max_epochs'],\n",
    "            device=config['device'],\n",
    "            wandb_config=wandb_config\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\nStarting training...\")\n",
    "        trainer.train(config['save_dir'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nTraining failed with error: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        # Clean up wandb\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 5973250,
     "sourceId": 52950,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
