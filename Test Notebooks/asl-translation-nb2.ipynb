{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":52950,"databundleVersionId":5973250,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install python-levenshtein tqdm pyarrow wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:53:03.524994Z","iopub.execute_input":"2024-11-29T08:53:03.525496Z","iopub.status.idle":"2024-11-29T08:53:13.921986Z","shell.execute_reply.started":"2024-11-29T08:53:03.525440Z","shell.execute_reply":"2024-11-29T08:53:13.920998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport random\nimport math\nfrom typing import List, Optional, Tuple, Dict\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nimport Levenshtein\nfrom tqdm.auto import tqdm\nimport time\nimport pyarrow.parquet as pq\nimport wandb\nimport sys\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef preprocess_data(\n    data_dir: str,\n    metadata_path: str,\n    output_dir: str,\n    chunk_size: int = 1000\n):\n    \"\"\"\n    Preprocess the ASL data into smaller, preprocessed chunks\n    \"\"\"\n    print(\"Loading metadata...\")\n    df = pd.read_csv(metadata_path)\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create sequence to file mapping\n    print(\"Creating sequence index...\")\n    sequence_map = {}\n    for parquet_file in tqdm(list(Path(data_dir).glob('*.parquet'))):\n        table = pq.read_table(parquet_file, columns=['sequence_id'])\n        sequences = pd.unique(table['sequence_id'].to_numpy())\n        for seq_id in sequences:\n            sequence_map[seq_id] = str(parquet_file)\n    \n    # Filter sequences\n    df = df[df['sequence_id'].isin(sequence_map.keys())]\n    \n    # Calculate number of chunks\n    num_chunks = (len(df) + chunk_size - 1) // chunk_size\n    \n    # Process data in chunks\n    for chunk_idx in range(num_chunks):\n        chunk_start = chunk_idx * chunk_size\n        chunk_end = min((chunk_idx + 1) * chunk_size, len(df))\n        chunk_df = df.iloc[chunk_start:chunk_end]\n        \n        # Process sequences in current chunk\n        processed_data = []\n        for _, row in tqdm(chunk_df.iterrows(), total=len(chunk_df), \n                          desc=f\"Processing chunk {chunk_idx+1}/{num_chunks}\"):\n            seq_id = row['sequence_id']\n            parquet_file = sequence_map[seq_id]\n            \n            # Read sequence data\n            table = pq.read_table(\n                parquet_file,\n                filters=[('sequence_id', '=', seq_id)]\n            )\n            seq_df = table.to_pandas()\n            \n            # Extract landmarks\n            landmark_cols = [col for col in seq_df.columns \n                           if col not in ['sequence_id', 'frame']]\n            landmarks = seq_df[landmark_cols].values\n            num_landmarks = len(landmark_cols) // 3\n            landmarks = landmarks.reshape(-1, num_landmarks, 3)\n            \n            # Store processed sequence\n            processed_data.append({\n                'sequence_id': seq_id,\n                'phrase': row['phrase'],\n                'landmarks': landmarks,\n                'participant_id': row['participant_id']\n            })\n        \n        # Save chunk\n        chunk_file = os.path.join(output_dir, f'chunk_{chunk_idx:04d}.pt')\n        torch.save(processed_data, chunk_file)\n        \n    # Save metadata\n    metadata = {\n        'num_chunks': num_chunks,\n        'chunk_size': chunk_size,\n        'total_sequences': len(df),\n        'participants': df['participant_id'].unique().tolist()\n    }\n    \n    with open(os.path.join(output_dir, 'metadata.json'), 'w') as f:\n        json.dump(metadata, f)\n    \n    print(f\"Preprocessing complete. {num_chunks} chunks saved to {output_dir}\")\n\nclass ASLTokenizer:\n    \"\"\"Tokenizer for ASL fingerspelling sequences\"\"\"\n    def __init__(self, vocab_path: str):\n        with open(vocab_path, 'r') as f:\n            self.char_to_idx = json.load(f)\n            \n        self.idx_to_char = {v: k for k, v in self.char_to_idx.items()}\n        self.vocab_size = len(self.char_to_idx)\n        self.pad_token = 0\n        self.sos_token = 1\n        self.eos_token = 2\n        \n    def encode(self, text: str) -> torch.Tensor:\n        \"\"\"Convert text to token indices\"\"\"\n        tokens = [self.sos_token]\n        for char in text:\n            tokens.append(self.char_to_idx.get(char, self.pad_token))\n        tokens.append(self.eos_token)\n        return torch.tensor(tokens)\n    \n    def decode(self, tokens: torch.Tensor) -> str:\n        \"\"\"Convert token indices to text\"\"\"\n        text = []\n        for token in tokens:\n            if token.item() == self.eos_token:\n                break\n            if token.item() not in [self.pad_token, self.sos_token]:\n                text.append(self.idx_to_char[token.item()])\n        return ''.join(text)\n\nclass ASLDataset(Dataset):\n    def __init__(\n        self,\n        processed_dir: str,\n        tokenizer: ASLTokenizer,\n        max_len: int = 384,\n        augment: bool = True,\n        fold: int = 0,\n        num_folds: int = 4,\n        mode: str = 'train'\n    ):\n        self.processed_dir = Path(processed_dir)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.augment = augment\n        \n        # Load metadata\n        with open(self.processed_dir / 'metadata.json', 'r') as f:\n            metadata = json.load(f)\n        \n        # Get all chunk files\n        self.chunk_files = sorted(list(self.processed_dir.glob('chunk_*.pt')))\n        \n        # Split participants for cross-validation\n        participants = metadata['participants']\n        np.random.seed(42)\n        np.random.shuffle(participants)\n        fold_size = len(participants) // num_folds\n        val_participants = participants[fold * fold_size:(fold + 1) * fold_size]\n        \n        # Load all chunks and filter by participant\n        self.data = []\n        for chunk_file in tqdm(self.chunk_files, desc='Loading chunks'):\n            chunk_data = torch.load(chunk_file)\n            for item in chunk_data:\n                is_val = item['participant_id'] in val_participants\n                if (mode == 'train' and not is_val) or (mode == 'val' and is_val):\n                    # Pre-process landmarks\n                    landmarks = torch.from_numpy(item['landmarks']).float()\n                    landmarks = self.normalize_landmarks(landmarks)\n                    tokens = self.tokenizer.encode(item['phrase'])\n                    \n                    self.data.append({\n                        'landmarks': landmarks,\n                        'tokens': tokens,\n                        'phrase': item['phrase']\n                    })\n        \n        print(f\"Dataset contains {len(self.data)} sequences\")\n    \n    def normalize_landmarks(self, landmarks: torch.Tensor) -> torch.Tensor:\n        \"\"\"Normalize landmarks efficiently\"\"\"\n        mean = landmarks.mean(dim=0, keepdim=True)\n        std = landmarks.std(dim=0, keepdim=True)\n        std[std < 1e-6] = 1.0\n        return (landmarks - mean) / std\n    \n    def augment_landmarks(self, landmarks: torch.Tensor) -> torch.Tensor:\n        \"\"\"Efficient landmark augmentation\"\"\"\n        T = landmarks.shape[0]\n        \n        # Combined random operations\n        if torch.rand(1) < 0.8:\n            # Time augmentation\n            scale = 0.8 + 0.4 * torch.rand(1)\n            new_T = int(T * scale)\n            if new_T > 0:\n                landmarks = F.interpolate(\n                    landmarks.permute(2, 0, 1)[None],\n                    size=new_T,\n                    mode='linear',\n                    align_corners=False\n                )[0].permute(1, 2, 0)\n                T = new_T\n        \n        # Spatial augmentation\n        if torch.rand(1) < 0.8:\n            angle = (torch.rand(1) * 60 - 30) * np.pi / 180\n            scale = 0.8 + 0.4 * torch.rand(1)\n            \n            # Apply rotation and scaling to x,y coordinates\n            cos_t, sin_t = torch.cos(angle), torch.sin(angle)\n            rot_matrix = torch.tensor([[cos_t, -sin_t], [sin_t, cos_t]]).float()\n            \n            xy_coords = landmarks[..., :2].reshape(-1, 2)\n            xy_coords = scale * (xy_coords @ rot_matrix.T)\n            landmarks[..., :2] = xy_coords.reshape(T, -1, 2)\n        \n        return landmarks\n    \n    def __len__(self) -> int:\n        return len(self.data)\n    \n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n        item = self.data[idx]\n        landmarks = item['landmarks']\n        \n        # Apply augmentation\n        if self.augment and torch.rand(1) < 0.2:\n            landmarks = self.augment_landmarks(landmarks)\n        \n        # Handle sequence length\n        T = landmarks.shape[0]\n        if T > self.max_len:\n            indices = torch.linspace(0, T-1, self.max_len).long()\n            landmarks = landmarks[indices]\n        else:\n            landmarks = F.pad(landmarks, (0, 0, 0, 0, 0, self.max_len - T))\n        \n        return {\n            'landmarks': landmarks,\n            'tokens': item['tokens'],\n            'phrase': item['phrase'],\n            'length': torch.tensor(T)\n        }\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, input_channels: int = 3, output_dim: int = 52):\n        super().__init__()\n        self.conv = nn.Conv1d(input_channels, 64, kernel_size=3, padding=1)\n        self.bn = nn.BatchNorm1d(64)\n        self.linear = nn.Linear(64, output_dim)\n\n    def forward(self, x):\n        B, T, L, C = x.shape\n        x = x.permute(0, 1, 3, 2)\n        x = x.reshape(B * T, C, L)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = F.relu(x)\n        x = x.mean(dim=2)\n        x = self.linear(x)\n        x = x.reshape(B, T, -1)\n        return x\n\nclass RotaryPositionalEmbedding(nn.Module):\n    def __init__(self, dim: int, max_seq_len: int = 384):\n        super().__init__()\n        head_dim = dim // 8\n        half_head_dim = head_dim // 2\n        emb = math.log(10000) / (half_head_dim - 1)\n        emb = torch.exp(torch.arange(half_head_dim) * -emb)\n        pos = torch.arange(max_seq_len)\n        emb = pos[:, None] * emb[None, :]\n        self.register_buffer('sin', emb.sin())\n        self.register_buffer('cos', emb.cos())\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        return self.sin[:seq_len], self.cos[:seq_len]\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.dim = dim\n        self.head_dim = dim // num_heads\n        assert self.head_dim * num_heads == dim\n        \n        self.q_proj = nn.Linear(dim, dim)\n        self.k_proj = nn.Linear(dim, dim)\n        self.v_proj = nn.Linear(dim, dim)\n        self.out_proj = nn.Linear(dim, dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def apply_rotary_pos_emb(self, q, k, sin, cos):\n        sin = sin.unsqueeze(0).unsqueeze(2)\n        cos = cos.unsqueeze(0).unsqueeze(2)\n        q1, q2 = q.chunk(2, dim=-1)\n        k1, k2 = k.chunk(2, dim=-1)\n        q = torch.cat([q1 * cos - q2 * sin, q2 * cos + q1 * sin], dim=-1)\n        k = torch.cat([k1 * cos - k2 * sin, k2 * cos + k1 * sin], dim=-1)\n        return q, k\n\n    def forward(self, x: torch.Tensor, sin: torch.Tensor, cos: torch.Tensor, \n                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        B, L, D = x.shape\n        \n        q = self.q_proj(x).reshape(B, L, self.num_heads, self.head_dim)\n        k = self.k_proj(x).reshape(B, L, self.num_heads, self.head_dim)\n        v = self.v_proj(x).reshape(B, L, self.num_heads, self.head_dim)\n        \n        q, k = self.apply_rotary_pos_emb(q, k, sin, cos)\n        \n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n        \n        scale = self.head_dim ** -0.5\n        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n        \n        if mask is not None:\n            mask = mask.unsqueeze(1).unsqueeze(2)\n            attn = attn.masked_fill(~mask, float('-inf'))\n        \n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        out = torch.matmul(attn, v)\n        out = out.transpose(1, 2).contiguous()\n        out = out.reshape(B, L, D)\n        \n        return self.out_proj(out)\n\nclass SqueezeformerBlock(nn.Module):\n    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):\n        super().__init__()\n        self.dim = dim\n        self.norm1 = nn.LayerNorm(dim)\n        self.mhsa = MultiHeadAttention(dim, num_heads, dropout)\n        \n        # Feed forward modules\n        self.ff1_norm = nn.LayerNorm(dim)\n        self.ff1 = nn.Sequential(\n            nn.Linear(dim, dim*4),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim*4, dim),\n            nn.Dropout(dropout)\n        )\n        \n        # Convolution module\n        self.conv_norm = nn.LayerNorm(dim)\n        self.conv1 = nn.Conv1d(dim, dim*2, 1)\n        self.glu = nn.GLU(dim=1)\n        self.depthwise_conv = nn.Conv1d(dim, dim, 3, padding=1, groups=dim)\n        self.batch_norm = nn.BatchNorm1d(dim)\n        self.activation = nn.SiLU()\n        self.pointwise_conv = nn.Conv1d(dim, dim, 1)\n        self.conv_dropout = nn.Dropout(dropout)\n        \n        # Feed forward module 2\n        self.ff2_norm = nn.LayerNorm(dim)\n        self.ff2 = nn.Sequential(\n            nn.Linear(dim, dim*4),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim*4, dim),\n            nn.Dropout(dropout)\n        )\n        \n        self.dropout = nn.Dropout(dropout)\n        self.scale = nn.Parameter(torch.ones(1))\n\n    def forward(self, x: torch.Tensor, sin: torch.Tensor, cos: torch.Tensor,\n                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        # First feed forward\n        residual = x\n        x = self.ff1_norm(x)\n        x = self.ff1(x)\n        x = residual + x * self.scale\n        \n        # Self attention\n        residual = x\n        x = self.norm1(x)\n        x = self.mhsa(x, sin, cos, mask)\n        x = self.dropout(x)\n        x = residual + x * self.scale\n        \n        # Convolution module\n        residual = x\n        x = self.conv_norm(x)\n        x = x.transpose(1, 2)\n        x = self.conv1(x)\n        x = self.glu(x)\n        x = self.depthwise_conv(x)\n        x = self.batch_norm(x)\n        x = self.activation(x)\n        x = self.pointwise_conv(x)\n        x = self.conv_dropout(x)\n        x = x.transpose(1, 2)\n        x = residual + x * self.scale\n        \n        # Second feed forward\n        residual = x\n        x = self.ff2_norm(x)\n        x = self.ff2(x)\n        x = residual + x * self.scale\n        \n        return x\n\nclass ASLTranslationModel(nn.Module):\n    def __init__(\n        self,\n        num_landmarks: int = 130,\n        feature_dim: int = 208,\n        num_classes: int = 59,\n        num_layers: int = 2,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        \n        # Feature extractors\n        self.face_extractor = FeatureExtractor(3, 52)\n        self.pose_extractor = FeatureExtractor(3, 52)\n        self.left_hand_extractor = FeatureExtractor(3, 52)\n        self.right_hand_extractor = FeatureExtractor(3, 52)\n        \n        # Embeddings\n        self.target_embedding = nn.Embedding(num_classes, feature_dim)\n        self.pos_embedding = RotaryPositionalEmbedding(feature_dim)\n        \n        # Squeezeformer encoder\n        self.squeezeformer_layers = nn.ModuleList([\n            SqueezeformerBlock(feature_dim, dropout=dropout)\n            for _ in range(num_layers)\n        ])\n        \n        # Decoder\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=feature_dim,\n            nhead=8,\n            dim_feedforward=feature_dim*4,\n            dropout=dropout,\n            batch_first=True,\n            norm_first=True\n        )\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n        \n        # Output layers\n        self.confidence_head = nn.Linear(feature_dim, 1)\n        self.classifier = nn.Linear(feature_dim, num_classes)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n        tgt: Optional[torch.Tensor] = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        B, T, L, C = x.shape\n        \n        # Extract features\n        face = x[:, :, :76]\n        pose = x[:, :, 76:88]\n        left_hand = x[:, :, 88:109]\n        right_hand = x[:, :, 109:]\n        \n        # Process each part\n        face_feats = self.face_extractor(face)\n        pose_feats = self.pose_extractor(pose)\n        left_hand_feats = self.left_hand_extractor(left_hand)\n        right_hand_feats = self.right_hand_extractor(right_hand)\n        \n        # Combine features\n        features = torch.cat([face_feats, pose_feats, left_hand_feats, right_hand_feats], dim=-1)\n        \n        # Get positional embeddings\n        sin, cos = self.pos_embedding(features)\n        \n        # Encoder\n        encoder_out = features\n        encoder_padding_mask = mask if mask is not None else None\n        \n        for layer in self.squeezeformer_layers:\n            encoder_out = layer(encoder_out, sin, cos, encoder_padding_mask)\n        \n        confidence = self.confidence_head(encoder_out[:, 0]).squeeze(-1)\n        \n        if tgt is not None:\n            # Teacher forcing during training\n            tgt_embedded = self.target_embedding(tgt)\n            tgt_embedded = self.dropout(tgt_embedded)\n            \n            tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(x.device)\n            memory_padding_mask = ~encoder_padding_mask if encoder_padding_mask is not None else None\n            \n            decoder_out = self.decoder(\n                tgt_embedded,\n                encoder_out,\n                tgt_mask=tgt_mask,\n                memory_key_padding_mask=memory_padding_mask\n            )\n            output = self.classifier(decoder_out)\n        else:\n            output = self.classifier(encoder_out)\n        \n        return output, confidence\n\n    @staticmethod\n    def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n        mask = mask.masked_fill(mask == 1, float('-inf'))\n        mask = mask.masked_fill(mask == 0, float(0.0))\n        return mask\n\nclass ASLTranslationLoss(nn.Module):\n    def __init__(self, pad_idx: int = 0):\n        super().__init__()\n        self.criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n        \n    def forward(\n        self,\n        pred: torch.Tensor,\n        target: torch.Tensor,\n        confidence: torch.Tensor,\n        confidence_target: torch.Tensor\n    ) -> torch.Tensor:\n        seq_loss = self.criterion(pred.reshape(-1, pred.size(-1)), target.reshape(-1))\n        conf_loss = F.mse_loss(confidence, confidence_target)\n        return seq_loss + 0.1 * conf_loss\n\ndef collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n    \"\"\"Custom collate function for batching\"\"\"\n    max_token_len = max(item['tokens'].size(0) for item in batch)\n    \n    landmarks = torch.stack([item['landmarks'] for item in batch])\n    tokens = torch.stack([\n        F.pad(item['tokens'], (0, max_token_len - item['tokens'].size(0)), value=0)\n        for item in batch\n    ])\n    lengths = torch.stack([item['length'] for item in batch])\n    \n    return {\n        'landmarks': landmarks,\n        'tokens': tokens,\n        'phrase': [item['phrase'] for item in batch],\n        'length': lengths\n    }\n\nclass Trainer:\n    def __init__(\n        self,\n        model: nn.Module,\n        train_loader: DataLoader,\n        val_loader: DataLoader,\n        tokenizer: ASLTokenizer,\n        learning_rate: float = 0.0045,\n        weight_decay: float = 0.08,\n        warmup_epochs: int = 1,\n        max_epochs: int = 2,\n        device: str = 'cuda',\n        wandb_config: dict = None\n    ):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.tokenizer = tokenizer\n        self.device = device\n        self.max_epochs = max_epochs\n        self.wandb_config = wandb_config\n        \n        if wandb_config:\n            wandb.init(\n                project=wandb_config['project'],\n                name=wandb_config['run_name'],\n                config={\n                    'learning_rate': learning_rate,\n                    'weight_decay': weight_decay,\n                    'warmup_epochs': warmup_epochs,\n                    'max_epochs': max_epochs,\n                    'batch_size': train_loader.batch_size,\n                    'architecture': 'Squeezeformer'\n                }\n            )\n            wandb.watch(model, log_freq=100)\n        \n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=learning_rate,\n            weight_decay=weight_decay\n        )\n        \n        self.num_training_steps = len(train_loader) * max_epochs\n        self.num_warmup_steps = len(train_loader) * warmup_epochs\n        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            self.optimizer,\n            max_lr=learning_rate,\n            total_steps=self.num_training_steps,\n            pct_start=self.num_warmup_steps / self.num_training_steps,\n            anneal_strategy='cos',\n            cycle_momentum=False\n        )\n        \n        self.criterion = ASLTranslationLoss()\n        self.scaler = GradScaler()\n    \n    def train_epoch(self) -> float:\n        self.model.train()\n        total_loss = 0\n        \n        progress_bar = tqdm(\n            self.train_loader,\n            desc='Training',\n            leave=True,\n            position=0,\n            dynamic_ncols=True\n        )\n        \n        for batch_idx, batch in enumerate(progress_bar):\n            self.optimizer.zero_grad()\n            \n            landmarks = batch['landmarks'].to(self.device)\n            tokens = batch['tokens'].to(self.device)\n            lengths = batch['length'].to(self.device)\n            \n            # Create mask\n            mask = torch.arange(landmarks.size(1), device=self.device)[None, :] < lengths[:, None]\n            \n            with autocast():\n                pred, confidence = self.model(landmarks, mask, tokens[:, :-1])\n                \n                with torch.no_grad():\n                    confidence_target = torch.tensor([\n                        1 - Levenshtein.distance(\n                            self.tokenizer.decode(p.argmax(-1).cpu()),\n                            true_text\n                        ) / max(len(true_text), 1)\n                        for p, true_text in zip(pred, batch['phrase'])\n                    ], device=self.device)\n                \n                loss = self.criterion(pred, tokens[:, 1:], confidence, confidence_target)\n            \n            self.scaler.scale(loss).backward()\n            self.scaler.unscale_(self.optimizer)\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            self.scheduler.step()\n            \n            total_loss += loss.item()\n            current_lr = self.optimizer.param_groups[0]['lr']\n            \n            if self.wandb_config and batch_idx % 10 == 0:\n                wandb.log({\n                    'batch_loss': loss.item(),\n                    'learning_rate': current_lr,\n                    'batch_confidence': confidence.mean().item(),\n                    'batch_confidence_target': confidence_target.mean().item()\n                })\n            \n            progress_bar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'lr': f'{current_lr:.2e}'\n            })\n        \n        return total_loss / len(self.train_loader)\n    \n    @torch.no_grad()\n    def validate(self) -> Tuple[float, float]:\n        self.model.eval()\n        total_loss = 0\n        predictions = []\n        ground_truth = []\n        confidence_scores = []\n        \n        for batch in tqdm(self.val_loader, desc='Validating'):\n            landmarks = batch['landmarks'].to(self.device)\n            tokens = batch['tokens'].to(self.device)\n            lengths = batch['length'].to(self.device)\n            \n            mask = torch.arange(landmarks.size(1), device=self.device)[None, :] < lengths[:, None]\n            \n            pred, confidence = self.model(landmarks, mask)\n            confidence_scores.extend(confidence.cpu().tolist())\n            \n            pred_texts = [self.tokenizer.decode(p.argmax(-1)) for p in pred]\n            predictions.extend(pred_texts)\n            ground_truth.extend(batch['phrase'])\n            \n            confidence_target = torch.tensor([\n                1 - Levenshtein.distance(pred_text, true_text) / max(len(true_text), 1)\n                for pred_text, true_text in zip(pred_texts, batch['phrase'])\n            ]).to(self.device)\n            \n            loss = self.criterion(pred, tokens[:, 1:], confidence, confidence_target)\n            total_loss += loss.item()\n        \n        avg_loss = total_loss / len(self.val_loader)\n        distances = [\n            1 - Levenshtein.distance(pred, true) / max(len(pred), len(true))\n            for pred, true in zip(predictions, ground_truth)\n        ]\n        avg_score = sum(distances) / len(distances)\n        \n        if self.wandb_config:\n            wandb.log({\n                'val_loss': avg_loss,\n                'val_score': avg_score,\n                'val_confidence_mean': np.mean(confidence_scores),\n                'val_confidence_std': np.std(confidence_scores)\n            })\n        \n        return avg_loss, avg_score\n    \n    def train(self, save_dir: str):\n        os.makedirs(save_dir, exist_ok=True)\n        best_val_score = float('-inf')\n        \n        for epoch in range(self.max_epochs):\n            print(f\"\\nEpoch {epoch + 1}/{self.max_epochs}\")\n            train_loss = self.train_epoch()\n            \n            if (epoch + 1) % 5 == 0:\n                val_loss, val_score = self.validate()\n                print(f\"Validation Loss: {val_loss:.4f}\")\n                print(f\"Validation Score: {val_score:.4f}\")\n                \n                if val_score > best_val_score:\n                    best_val_score = val_score\n                    torch.save({\n                        'epoch': epoch,\n                        'model_state_dict': self.model.state_dict(),\n                        'optimizer_state_dict': self.optimizer.state_dict(),\n                        'scheduler_state_dict': self.scheduler.state_dict(),\n                        'val_score': val_score,\n                    }, os.path.join(save_dir, 'best_model.pt'))\n            \n            if (epoch + 1) % 40 == 0:\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'scheduler_state_dict': self.scheduler.state_dict(),\n                }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pt'))\n\ndef main():\n    # Configuration\n    config = {\n        'data_dir': '/kaggle/input/asl-fingerspelling/train_landmarks',\n        'metadata_path': '/kaggle/input/asl-fingerspelling/train.csv',\n        'vocab_path': '/kaggle/input/asl-fingerspelling/character_to_prediction_index.json',\n        'processed_dir': '/kaggle/working/processed_data',\n        'save_dir': '/kaggle/working/models',\n        'batch_size': 128,\n        'max_len': 384,\n        'num_workers': 8,\n        'learning_rate': 0.003,\n        'weight_decay': 0.08,\n        'warmup_epochs': 1,\n        'max_epochs': 15,\n        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n    }\n    \n    # Set random seeds\n    torch.manual_seed(42)\n    random.seed(42)\n    np.random.seed(42)\n    \n    # Create directories\n    os.makedirs(config['processed_dir'], exist_ok=True)\n    os.makedirs(config['save_dir'], exist_ok=True)\n    \n    # Preprocess data if not already done\n    if not os.path.exists(os.path.join(config['processed_dir'], 'metadata.json')):\n        print(\"Preprocessing data...\")\n        preprocess_data(\n            config['data_dir'],\n            config['metadata_path'],\n            config['processed_dir']\n        )\n    \n    # Initialize tokenizer and datasets\n    print(\"\\nInitializing tokenizer and datasets...\")\n    tokenizer = ASLTokenizer(config['vocab_path'])\n    \n    train_dataset = ASLDataset(\n        config['processed_dir'],\n        tokenizer,\n        max_len=config['max_len'],\n        augment=True,\n        mode='train'\n    )\n    \n    val_dataset = ASLDataset(\n        config['processed_dir'],\n        tokenizer,\n        max_len=config['max_len'],\n        augment=False,\n        mode='val'\n    )\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers'],\n        pin_memory=True,\n        persistent_workers=True,\n        collate_fn=collate_fn\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers'],\n        pin_memory=True,\n        persistent_workers=True,\n        collate_fn=collate_fn\n    )\n    \n    # Initialize model\n    model = ASLTranslationModel(\n        num_landmarks=130,\n        feature_dim=208,\n        num_classes=len(tokenizer.char_to_idx),\n        num_layers=2,\n        dropout=0.1\n    )\n    \n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n    \n    # Initialize trainer\n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        tokenizer=tokenizer,\n        learning_rate=config['learning_rate'],\n        weight_decay=config['weight_decay'],\n        warmup_epochs=config['warmup_epochs'],\n        max_epochs=config['max_epochs'],\n        device=config['device'],\n        wandb_config={\n            'project': 'asl-translation',\n            'run_name': f'asl-translation-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n        }\n    )\n    \n    # Train model\n    print(\"\\nStarting training...\")\n    trainer.train(config['save_dir'])\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T08:53:13.924161Z","iopub.execute_input":"2024-11-29T08:53:13.924471Z"}},"outputs":[],"execution_count":null}]}